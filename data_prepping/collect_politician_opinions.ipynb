{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import logging\n",
    "import torch\n",
    "from transformers import AutoTokenizer, BertForSequenceClassification\n",
    "from params.paths import ROOT_DIR, CHROMEDRIVER_PATH\n",
    "from api_requests.meeting_convo_collector import MeetingConvoCollector\n",
    "\n",
    "from file_handling.file_read_writer import read_json, write_json, create_dir, write_file\n",
    "\n",
    "OUTPUT_DIR = os.path.join(ROOT_DIR, 'data', 'data_repr_upper')\n",
    "SPEECHES_LOWER_DIR = os.path.join(ROOT_DIR, 'data', 'data_repr_lower')\n",
    "SPEECHES_UPPER_DIR = os.path.join(ROOT_DIR, 'data', 'data_repr_upper')\n",
    "create_dir(OUTPUT_DIR)\n",
    "print(os.path.abspath(OUTPUT_DIR))\n",
    "LOWER_HOUSE_DATA_DIR = os.path.join(ROOT_DIR, 'data', 'data_shugiin')\n",
    "UPPER_HOUSE_DATA_DIR = os.path.join(ROOT_DIR, 'data', 'data_sangiin')\n",
    "\n",
    "#reading the reprentative data for lower and upper house\n",
    "lower_repr_dir = os.path.join(LOWER_HOUSE_DATA_DIR, 'repr_list')\n",
    "lower_repr_file = os.listdir(lower_repr_dir)[0]\n",
    "lower_house_meeting_dict = read_json(os.path.join(lower_repr_dir, lower_repr_file))\n",
    "lower_repr_dict = lower_house_meeting_dict['reprs']\n",
    "\n",
    "upper_repr_dir = os.path.join(UPPER_HOUSE_DATA_DIR, 'repr_list')\n",
    "upper_repr_file = os.listdir(upper_repr_dir)[0]\n",
    "upper_house_meeting_dict = read_json(os.path.join(upper_repr_dir, upper_repr_file))\n",
    "upper_repr_dict = upper_house_meeting_dict['reprs']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_repr_name(repr_name):\n",
    "\trepr_name = re.sub('\\s|君|\\[(.*?)\\]', '', repr_name)\n",
    "\treturn repr_name\n",
    "\n",
    "def remove_duplicate_speeches(speeches):\n",
    "\tids = []\n",
    "\tunique_speeches = []\n",
    "\tfor speech in speeches:\n",
    "\t\tif speech['speech_id'] not in ids:\n",
    "\t\t\tids.append(speech['speech_id'])\n",
    "\t\t\tunique_speeches.append(speech)\n",
    "\treturn unique_speeches\n",
    "\n",
    "\n",
    "class ReprTopicOpinionCollector:\n",
    "\tdef __init__(self, house='lower'):\n",
    "\t\tself.mcc = MeetingConvoCollector(\"https://kokkai.ndl.go.jp/api/speech?\")\n",
    "\t\tself.topic_dict = read_json(os.path.join(ROOT_DIR, 'resource','experiment_config.json'))\n",
    "\t\tif house == 'lower':\n",
    "\t\t\tself.repr_dict = lower_repr_dict\n",
    "\t\telif house == 'upper':\n",
    "\t\t\tself.repr_dict = upper_repr_dict\n",
    "\n",
    "\t\tself.model_name = \"kkatodus/jp-speech-classifier\"\n",
    "\t\tself.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "\t\tself.model = BertForSequenceClassification.from_pretrained(self.model_name)\n",
    "\n",
    "\t\tlog_dir = os.path.join(ROOT_DIR, 'logs')\n",
    "\t\tcreate_dir(log_dir)\n",
    "\t\tlogging.basicConfig(filename=os.path.join(log_dir, 'politician_opinion_collection.log'), filemode='w', format='%(asctime)s - %(message)s')\n",
    "\t\tself.logger = logging.getLogger()\n",
    "\t\tself.logger.setLevel(logging.DEBUG)\n",
    "\n",
    "\t\tself.current_party = None\n",
    "\t\tself.current_repr = None\n",
    "\t\tself.current_topic = None\n",
    "\t\tself.current_search_words = None\n",
    "\t\tself.current_search_word = None\n",
    "\t\tself.current_speeches_dict_for_repr_for_topic = []\n",
    "\t\n",
    "\tdef check_search_words_in_string(self, string):\n",
    "\t\tfor search_word in self.current_search_words:\n",
    "\t\t\tif search_word in string:\n",
    "\t\t\t\treturn True\n",
    "\t\treturn False\n",
    "\t\n",
    "\tdef create_mini_batches_from_sentences(self, sentences, batch_size=100):\n",
    "\t\tmini_batches = []\n",
    "\t\tfor i in range(0, len(sentences), batch_size):\n",
    "\t\t\tmini_batches.append(sentences[i:i+batch_size])\n",
    "\t\treturn mini_batches\n",
    "\n",
    "\tdef extract_opinions(self, speech, target_class = ['意見文']):\n",
    "\t\tspeech_segments = speech.split('。')\n",
    "\t\tsegment_batches = self.create_mini_batches_from_sentences(speech_segments)\n",
    "\t\tself.logger.info(f\"Created {len(segment_batches)} speech segment batches of length {[len(batch) for batch in segment_batches]}\")\n",
    "\t\textracted_segments = []\n",
    "\t\tfor idx, segment_batch in enumerate(segment_batches):\n",
    "\t\t\tself.logger.info(f\"Encoding {len(segment_batch)} speech segments\")\n",
    "\t\t\tencoded = self.tokenizer(segment_batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "\t\t\twith torch.no_grad():\n",
    "\t\t\t\tself.logger.info(f\"Predicting {len(segment_batch)} speech segments\")\n",
    "\t\t\t\tlogits = self.model(**encoded).logits\n",
    "\t\t\tpredicted_class_id = logits.argmax(dim=1)\n",
    "\t\t\tclasses = [self.model.config.id2label[pred_id.item()] for pred_id in list(predicted_class_id)]\n",
    "\t\t\tfor idx, (sentence, pred_class) in enumerate(zip(segment_batch, classes)):\n",
    "\t\t\t\tif pred_class in target_class and self.check_search_words_in_string(sentence):\n",
    "\t\t\t\t\textracted_segments.extend([sentence])\n",
    "\t\tif len(extracted_segments) == 0:\n",
    "\t\t\tself.logger.info(f\"no opinion found for in speech segments with search word {self.current_search_word}\\n\\n\\n\")\n",
    "\t\t\n",
    "\t\treturn extracted_segments\n",
    "\n",
    "\tdef iterate_speeches(self, record):\n",
    "\t\toutput_array = []\n",
    "\t\tif record['numberOfRecords'] == 0:\n",
    "\t\t\treturn output_array\n",
    "\t\tfor idx, speech in enumerate(record['speechRecord']):\n",
    "\t\t\tself.logger.info(f\"Working on {idx}/{len(record['speechRecord'])} speech record\")\n",
    "\t\t\tspeech_id = speech['speechID']\n",
    "\t\t\thouse_name = speech['nameOfHouse']\n",
    "\t\t\tmeeting_name = speech['nameOfMeeting']\n",
    "\t\t\tdate = speech['date']\n",
    "\t\t\tspeech_text = speech['speech']\n",
    "\t\t\tspeech_url = speech['speechURL']\n",
    "\t\t\tspeaker_group = speech['speakerGroup']\n",
    "\t\t\textracted_opinions = self.extract_opinions(speech_text)\n",
    "\t\t\tif len(extracted_opinions) > 0:\n",
    "\t\t\t\t# speech_dict = {'speech_id': speech_id, 'house_name': house_name, 'meeting_name': meeting_name, 'date': date, 'speech_text': speech_text, 'speech_url': speech_url, 'speaker_group':speaker_group,'extracted_opinions': extracted_opinions}\n",
    "\t\t\t\tspeech_dict = {'speech_id': speech_id, 'house_name': house_name, 'meeting_name': meeting_name, 'date': date, 'speech_url': speech_url, 'speaker_group':speaker_group,'extracted_opinions': extracted_opinions}\n",
    "\t\t\t\toutput_array.append(speech_dict)\n",
    "\t\treturn output_array\n",
    "\n",
    "\tdef add_processed_speeches(self):\n",
    "\t\tconditions_list = [f\"any={self.current_search_word}\",f\"speaker={self.current_repr_name}\",'recordPacking=json','maximumRecords=50']\n",
    "\t\tstart_point = 1\n",
    "\t\tself.logger.info(f\"searching for {self.current_repr_name} with search word {self.current_search_word} in {self.current_topic} with start point {start_point}\")\n",
    "\n",
    "\t\twhile True:\n",
    "\t\t\tif start_point is None:\n",
    "\t\t\t\tbreak\n",
    "\t\t\tself.logger.info(f\"Making one request with start point {start_point}\")\n",
    "\t\t\tspeech_records, start_point = self.mcc.make_one_request(conditions_list, starting_point=start_point)\n",
    "\t\t\tself.logger.info(f\"Got {speech_records['numberOfRecords']} speeches records\")\n",
    "\t\t\tprocessed_speeches = self.iterate_speeches(speech_records)\n",
    "\t\t\tif len(processed_speeches) == 0:\n",
    "\t\t\t\tself.logger.info(\"No processed_speeches found for speech record\")\n",
    "\t\t\t\tcontinue\n",
    "\t\t\tself.current_speeches_dict_for_repr_for_topic.extend(processed_speeches)\n",
    "\t\t\tself.logger.info(f\"Added {len(processed_speeches)} speeches to the list with {len(self.current_speeches_dict_for_repr_for_topic)} speeches in total\")\n",
    "\n",
    "\tdef collect(self):\n",
    "\t\tfor party in self.repr_dict.keys():\n",
    "\t\t\tself.current_party = party\n",
    "\t\t\tfor repr in self.repr_dict[party]:\n",
    "\t\t\t\tself.current_repr = repr\n",
    "\t\t\t\tself.current_repr_name = clean_repr_name(repr['name'])\n",
    "\t\t\t\tfor topic_config in self.topic_dict:\n",
    "\t\t\t\t\ttopic = topic_config['topic_name']\n",
    "\t\t\t\t\tsearch_words = topic_config['search_words']\n",
    "\t\t\t\t\tself.current_topic = topic\n",
    "\t\t\t\t\tself.current_search_words = search_words\n",
    "\t\t\t\t\trepr_topic_dir = os.path.join(OUTPUT_DIR, party, self.current_repr_name, topic)\n",
    "\t\t\t\t\ttopic_file_path = os.path.join(repr_topic_dir, 'opinions.json')\n",
    "\t\t\t\t\tif os.path.exists(topic_file_path):\n",
    "\t\t\t\t\t\tprint('Already collected speeches for',party, self.current_repr_name, topic)\n",
    "\t\t\t\t\t\tcontinue\n",
    "\t\t\t\t\tprint(f\"Collecting speeches for {self.current_repr_name} with topic {self.current_topic}\")\n",
    "\t\t\t\t\tfor search_word in search_words:\n",
    "\t\t\t\t\t\tself.current_search_word = search_word\n",
    "\t\t\t\t\t\tself.add_processed_speeches()\n",
    "\t\t\t\t\tcreate_dir(repr_topic_dir)\n",
    "\t\t\t\t\tif len(self.current_speeches_dict_for_repr_for_topic) > 0:\n",
    "\t\t\t\t\t\tself.current_speeches_dict_for_repr_for_topic = remove_duplicate_speeches(self.current_speeches_dict_for_repr_for_topic)\n",
    "\t\t\t\t\t\tsorted_speeches = sorted(self.current_speeches_dict_for_repr_for_topic, key=lambda k: k['date'], reverse=True)\n",
    "\t\t\t\t\t\tout_dict = {'party': self.current_party, 'repr_name': self.current_repr_name, 'topic': self.current_topic, 'search_words': self.current_search_words, 'speeches': sorted_speeches}\n",
    "\t\t\t\t\t\tself.logger.info(f\"writing speeches for {self.current_repr_name} with search word {self.current_search_word} in {self.current_topic}\")\n",
    "\t\t\t\t\t\twrite_json(out_dict, topic_file_path)\n",
    "\t\t\t\t\t\tself.logger.info(f'Finished writing file')\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\twrite_json({}, topic_file_path)\n",
    "\t\t\t\t\t\tself.logger.info(f\"no speeches found for {self.current_repr_name} with search word {self.current_search_word} in {self.current_topic}\")\n",
    "\t\t\t\t\tself.current_speeches_dict_for_repr_for_topic = []\n",
    "\t\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script to collect opinion based sentences for each topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repr_topic_opinion_collector = ReprTopicOpinionCollector(house=\"upper\")\n",
    "repr_topic_opinion_collector.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating summary json to record topics for each politicians and how many files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a summary json for the repr opinions data\n",
    "dicts = [lower_repr_dict, upper_repr_dict]\n",
    "output_dirs = [SPEECHES_LOWER_DIR, SPEECHES_UPPER_DIR]\n",
    "for repr_dict, output_dir in zip(dicts, output_dirs):\n",
    "\tsummary_dict = {'reprs':{}}\n",
    "\n",
    "\tfor party in repr_dict.keys():\n",
    "\t\tsummary_dict['reprs'][party] = {}\n",
    "\t\tfor repr in repr_dict[party]:\n",
    "\t\t\trepr_name = repr['name']\n",
    "\t\t\trepr_name = clean_repr_name(repr_name)\n",
    "\t\t\trepr_dir_path = os.path.join(output_dir, party, repr_name)\n",
    "\t\t\tif not os.path.exists(repr_dir_path):\n",
    "\t\t\t\tcontinue\n",
    "\t\t\ttags = [dirname for dirname in os.listdir(repr_dir_path) if read_json(os.path.join(repr_dir_path, dirname, 'opinions.json')) != {}]\n",
    "\t\t\tif len(tags) == 0:\n",
    "\t\t\t\tcontinue\n",
    "\t\t\tsummary_dict['reprs'][party][repr_name] = {}\n",
    "\t\t\tsummary_dict['reprs'][party][repr_name]['tags'] = tags\n",
    "\twrite_json(summary_dict, os.path.join(output_dir, 'summary.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kokkai_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
