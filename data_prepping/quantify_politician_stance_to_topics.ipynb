{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About the code\n",
    "This code is going to be the main to collect and create analysis output to answer the question \"Quantifying political stance of Japanese Diet members regards specific political topics through the use of LLMs and statistical methods.\" (Tentative) \n",
    "\n",
    "## Part 1: Procedure for measuring embeddings\n",
    "1. Create embeddings for each opinion-based sentence in regards to different topics and store it in a retrievable manner.\n",
    "2. Create one single \"opinion-embedding\" for each politician and store it in a retrievable manner.\n",
    "3. Create a stance axis vector by either generating two reference points or picking two points \"opinion-embeddings\" from the data\n",
    "4. Collapse all the other vectors onto this axis by projecting them onto the axis\n",
    "5. Create a scalar measurement for how far each politician is from the two reference points\n",
    "\n",
    "## Part 2: Creating groups of politicians based on where their embeddings lie on an axis \n",
    "This is to get an idea of the ideas mentioned by politicians in a axis group within each topic using BERTopic. \n",
    "1. Divide politicians into n groups based on where they lie on the axis.\n",
    "2. Extract opinion sentences of each group made. \n",
    "3. Run each group through topic modelling techniques.\n",
    "\n",
    "### Notes\n",
    "- Data is stored under `data/data_repr` directory\n",
    "- We will attempt the procedure with different models to seek the best output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "DATA_DIR:  /root/projects/kokkai_analysis/data_prepping/params/../data\n",
      "DATA_REPR_SPEECHES_DIR:  /root/projects/kokkai_analysis/data_prepping/params/../data/data_repr_new\n",
      "PARTIES:  ['自民', '国民', '公明', '立憲', 'れ新', '維新', '無', '共産', '有志']\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from sentence_transformers import models, SentenceTransformer\n",
    "import h5py\n",
    "import numpy as np\n",
    "import torch\n",
    "from params.paths import ROOT_DIR\n",
    "import japanize_matplotlib \n",
    "import matplotlib.pyplot as plt\n",
    "from logger.Logger import Logger\n",
    "from file_handling.file_read_writer import read_json, write_json, create_dir, write_file, read_hdf5_file\n",
    "\n",
    "VERBOSE = True\n",
    "logger = Logger(verbose=VERBOSE)\n",
    "DATA_DIR = os.path.join(ROOT_DIR, 'data')\n",
    "DATA_REPR_SPEECHES_DIR = os.path.join(DATA_DIR, 'data_repr_new')\n",
    "RESOURCE_DIR = os.path.join(ROOT_DIR, 'resource')\n",
    "EXPERIMENT_CONFIG_PATH = os.path.join(RESOURCE_DIR, 'experiment_config.json')\n",
    "EXPERIMENT_CONFIG = read_json(EXPERIMENT_CONFIG_PATH)\n",
    "PARTIES = [party for party in os.listdir(DATA_REPR_SPEECHES_DIR) if not '.' in party]\n",
    "PARTY_TO_COLOR = {\n",
    "\t'自民': 'black',\n",
    "\t'国民': 'blue',\n",
    "\t'立憲': 'orange',\n",
    "\t'公明': 'lightblue',\n",
    "\t'共産': 'red',\n",
    "\t'維新': 'gold',\n",
    "\t'れ新': 'green',\n",
    "\t'無': 'purple',\n",
    "\t'有志': 'grey'\n",
    "}\n",
    "PARTY_TO_IDX = {party: idx for idx, party in enumerate(PARTIES)}\n",
    "IDX_TO_PARTY = {idx: party for idx, party in enumerate(PARTIES)}\n",
    "if len(PARTIES) != len(PARTY_TO_COLOR):\n",
    "\traise ValueError('PARTIES and PARTY_TO_COLOR must have the same length.')\n",
    "MODEL_NAME = \"cl-tohoku/bert-base-japanese-v3\"\n",
    "print('-----------------------------------')\n",
    "print('DATA_DIR: ', DATA_DIR)\n",
    "print('DATA_REPR_SPEECHES_DIR: ', DATA_REPR_SPEECHES_DIR)\n",
    "print('PARTIES: ', PARTIES)\n",
    "print('-----------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-v3 were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# adapted from: https://osima.jp/posts/sentence-bert/\n",
    "\n",
    "sentence_transformer = models.Transformer(MODEL_NAME)\n",
    "\n",
    "pooling = models.Pooling(\n",
    "    sentence_transformer.get_word_embedding_dimension(),\n",
    "    pooling_mode_mean_tokens=False,\n",
    "    pooling_mode_cls_token=True,\n",
    "    pooling_mode_max_tokens=False)\n",
    "\n",
    "st = SentenceTransformer(modules=[sentence_transformer, pooling])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Creating the embeddings for each opinion-based sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_speeches(speeches):\n",
    "\tencoded_opinions = st.encode(speeches, convert_to_tensor=True, show_progress_bar=True)\n",
    "\treturn encoded_opinions\n",
    "\n",
    "def read_opinion_sentences_and_dates(file_path):\n",
    "\tlogger.message(f'Reading {file_path}')\n",
    "\ttarget_dict = read_json(file_path)\n",
    "\tif not target_dict:\n",
    "\t\treturn [], []\n",
    "\topinion_sentences = []\n",
    "\tdates = []\n",
    "\tfor speech in target_dict['speeches']:\n",
    "\t\tdate = [speech['date'] for _ in range(len(speech['extracted_opinions']))]\n",
    "\t\topinions = speech['extracted_opinions']\n",
    "\t\topinion_sentences.extend(opinions)\n",
    "\t\tdates.extend(date)\n",
    "\t\n",
    "\treturn opinion_sentences, dates\n",
    "\n",
    "def iterate_topics_for_repr(repr_path, topics=[]):\n",
    "\tfor topic in os.listdir(repr_path):\n",
    "\t\tif topic not in topics and topics:\n",
    "\t\t\tcontinue\n",
    "\t\tlogger.message(f'Working on {topic}')\n",
    "\t\ttopic_path = os.path.join(repr_path, topic)\n",
    "\t\tif os.path.exists(os.path.join(topic_path, 'embeddings.hdf5')):\n",
    "\t\t\tlogger.message(f'Embeddings already exist for {topic} in {repr_path}')\n",
    "\t\t\tcontinue\n",
    "\t\tfile_path = os.path.join(topic_path, 'opinions.json')\n",
    "\t\ttopic_opinions, topic_dates = read_opinion_sentences_and_dates(file_path)\n",
    "\t\tif not topic_opinions:\n",
    "\t\t\tlogger.message(f'No opinions found for {topic} in {repr_path}')\n",
    "\t\t\tcontinue\n",
    "\t\tembeddings = embed_speeches(topic_opinions)\n",
    "\t\tembeddings = [embedding.cpu() for embedding in embeddings]\n",
    "\t\tlogger.message(f'Number of dates {len(topic_dates)}\\nNumber of opinions {len(topic_opinions)} \\nNumber of embeddings {len(embeddings)}')\n",
    "\t\tembeddings = torch.stack(embeddings)\n",
    "\t\twith h5py.File(os.path.join(topic_path, 'embeddings.hdf5'), 'w') as f:\n",
    "\t\t\tf.create_dataset('embeddings', data=embeddings)\n",
    "\t\t\tf.create_dataset('dates', data=topic_dates, dtype=h5py.string_dtype(encoding='utf-8'))\n",
    "\t\t\tf.create_dataset('opinions', data=topic_opinions, dtype=h5py.string_dtype(encoding='utf-8'))\n",
    "\n",
    "TOPICS_TO_CREATE_EMBEDDINGS = ['LGBTQ', '原発', '少子化', '気候変動', '経済対策', '防衛']\n",
    "for party in PARTIES:\n",
    "\tparty_path = os.path.join(DATA_REPR_SPEECHES_DIR, party)\n",
    "\trepr_names = os.listdir(party_path)\n",
    "\tfor repr_name in repr_names:\n",
    "\t\tlogger.message(f'{party} ----- {repr_name}')\n",
    "\t\trepr_path = os.path.join(party_path, repr_name)\n",
    "\t\titerate_topics_for_repr(repr_path, topics=TOPICS_TO_CREATE_EMBEDDINGS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create one single \"opinion-embedding\" for each politician and store it in a retrievable manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingHandler:\n",
    "\tdef __init__(self, data_dir=DATA_DIR, speeches_dir = DATA_REPR_SPEECHES_DIR, parties=PARTIES):\n",
    "\t\tself.data_dir = data_dir\n",
    "\t\tself.speeches_dir = speeches_dir\n",
    "\t\tself.parties = parties\n",
    "\t\tself.reprs = []\n",
    "\t\tself.repr2party = {}\n",
    "\t\tfor party in self.parties:\n",
    "\t\t\treprs = os.listdir(os.path.join(self.speeches_dir, party))\n",
    "\t\t\tself.reprs.extend(reprs)\n",
    "\t\t\tfor repr in reprs:\n",
    "\t\t\t\tself.repr2party[repr] = party\n",
    "\n",
    "\tdef check_party_exists(self, party):\n",
    "\t\tif party not in self.parties:\n",
    "\t\t\traise ValueError(f'{party} not in {self.parties}')\n",
    "\t\t\n",
    "\tdef get_reprs_for_party(self, party):\n",
    "\t\tself.check_party_exists(party)\n",
    "\t\treturn os.listdir(os.path.join(self.speeches_dir, party))\n",
    "\n",
    "\tdef get_topics_for_repr(self, party, repr_name):\n",
    "\t\tself.check_party_exists(party)\n",
    "\t\treturn os.listdir(os.path.join(self.speeches_dir, party, repr_name))\n",
    "\n",
    "\tdef get_embeddings_for_repr_for_topic(self,repr_name, topic, party=None):\n",
    "\t\tif party is None:\n",
    "\t\t\tparty = self.repr2party[repr_name]\n",
    "\t\tpath = os.path.join(self.speeches_dir, party, repr_name, topic, 'embeddings.hdf5')\n",
    "\t\tif not os.path.exists(path):\n",
    "\t\t\traise ValueError(f'{path} does not exist')\n",
    "\t\t\n",
    "\t\twith h5py.File(path, 'r') as f:\n",
    "\t\t\tembeddings = f['embeddings'][:]\n",
    "\t\t\tdates = [date.decode('utf-8') for date in f['dates'][:]]\n",
    "\t\t\topinions = [opinion.decode('utf-8') for opinion in f['opinions'][:]]\n",
    "\t\t\treturn embeddings, dates, opinions\n",
    "\t\t\n",
    "\tdef get_average_embedding_for_repr_for_topic(self, repr_name, topic, party=None):\n",
    "\t\tif party is None:\n",
    "\t\t\tparty = self.repr2party[repr_name]\n",
    "\t\tembeddings, _, _= self.get_embeddings_for_repr_for_topic(repr_name, topic, party=party)\n",
    "\t\treturn np.mean(embeddings, axis=0)\n",
    "\t\n",
    "\tdef create_summary_hdf5_file_for_average_embeddings(self, path, topic, reprs=None):\n",
    "\t\tif reprs is None:\n",
    "\t\t\treprs = self.reprs\n",
    "\t\tembeddings = []\n",
    "\t\treprs_with_embeddings = []\n",
    "\t\tfor repr in reprs:\n",
    "\t\t\ttry:\n",
    "\t\t\t\taverage_embedding = self.get_average_embedding_for_repr_for_topic(repr, topic)\n",
    "\t\t\t\tembeddings.append(average_embedding)\n",
    "\t\t\t\treprs_with_embeddings.append(repr)\n",
    "\t\t\texcept ValueError:\n",
    "\t\t\t\tlogger.message(f'No embeddings for {repr} for {topic}')\n",
    "\t\t\t\tcontinue\n",
    "\n",
    "\t\tembeddings = np.array(embeddings)\n",
    "\t\twith h5py.File(path, 'w') as f:\n",
    "\t\t\tf.create_dataset('embeddings', data=embeddings)\n",
    "\t\t\tf.create_dataset('reprs', data=reprs_with_embeddings, dtype=h5py.string_dtype(encoding='utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating summary hdf5 file for average embeddings for some topics for reprs where the data is available\n",
    "eh = EmbeddingHandler()\n",
    "TOPICS_TO_CREATE_AVERAGE_EMBEDDINGS = ['LGBTQ', '原発', '少子化', '気候変動', '経済対策', '防衛']\n",
    "for topic in TOPICS_TO_CREATE_AVERAGE_EMBEDDINGS:\n",
    "\tprint(f'Creating summary hdf5 file for all embeddings for one topic {topic}')\n",
    "\teh.create_summary_hdf5_file_for_average_embeddings(path = os.path.join(DATA_REPR_SPEECHES_DIR, f'{topic}.hdf5'),\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\ttopic = topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create a stance axis vector by either generating two reference points or picking two points \"opinion-embeddings\" from the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class VectorOperator:\n",
    "\tdef __init__(self):\n",
    "\t\tself.embedding_handler = EmbeddingHandler()\n",
    "\t\n",
    "\tdef project_vector(vector, onto_vector):\n",
    "\t\tnormalized_onto_vector = onto_vector / np.linalg.norm(onto_vector)\n",
    "\t\tscaling = np.dot(vector, normalized_onto_vector)\n",
    "\t\tprojection = scaling * normalized_onto_vector\n",
    "\t\treturn projection, scaling\n",
    "\t\n",
    "\tdef get_embeddings_and_reprs(self, summary_hdf5_path):\n",
    "\t\thdf5_dict = read_hdf5_file(summary_hdf5_path)\n",
    "\t\tembeddings = hdf5_dict['embeddings'][:]\n",
    "\t\treprs = [repr.decode('utf-8') for repr in hdf5_dict['reprs'][:]]\n",
    "\t\treturn embeddings, reprs\n",
    "\t\n",
    "\tdef collapse_vectors_onto_two_ref_reprs(self, summary_hdf5_path, topic, ref_repr1, ref_repr2):\n",
    "\t\tembeddings, reprs = self.get_embeddings_and_reprs(summary_hdf5_path)\n",
    "\t\tif ref_repr1 not in reprs:\n",
    "\t\t\traise ValueError(f'{ref_repr1} not in {reprs}')\n",
    "\t\tif ref_repr2 not in reprs:\n",
    "\t\t\traise ValueError(f'{ref_repr2} not in {reprs}')\n",
    "\t\tref1_embedding = self.embedding_handler.get_average_embedding_for_repr_for_topic(ref_repr1, topic)\n",
    "\t\tref2_embedding = self.embedding_handler.get_average_embedding_for_repr_for_topic(ref_repr2, topic)\n",
    "\t\tref1_to_ref2 = ref2_embedding - ref1_embedding\n",
    "\t\tprojections = embeddings @ ref1_to_ref2\n",
    "\t\tprojections = projections / np.linalg.norm(ref1_to_ref2)\n",
    "\t\treturn projections, reprs\n",
    "\n",
    "\tdef collapse_vectors_onto_two_genenerated_strings(self, summary_hdf5_path, topic, string1, string2):\n",
    "\t\tembeddings, reprs = self.get_embeddings_and_reprs(summary_hdf5_path)\n",
    "\t\tstring1_embedding = st.encode(string1, convert_to_tensor=True, show_progress_bar=True)\n",
    "\t\tstring2_embedding = st.encode(string2, convert_to_tensor=True, show_progress_bar=True)\n",
    "\t\tstring1_embedding = string1_embedding.cpu().numpy()\n",
    "\t\tstring2_embedding = string2_embedding.cpu().numpy()\n",
    "\t\tstring1_to_string2 = string2_embedding - string1_embedding\n",
    "\t\tprojections = embeddings @ string1_to_string2\n",
    "\t\tprojections = projections / np.linalg.norm(string1_to_string2)\n",
    "\t\treturn projections, reprs\n",
    "\t\n",
    "class PoliticalStanceVisualizer:\n",
    "\tdef __init__(self):\n",
    "\t\tpass\n",
    "\n",
    "\tdef plot_grouped_bar_chart(self, ax, xs, party, xmax, xmin, title, xlabel='', ylabel=\"\", color=\"blue\"):\n",
    "\t\tax.set_title(title)\n",
    "\t\tax.set_xlabel(xlabel)\n",
    "\t\tax.set_ylabel(ylabel)\n",
    "\t\tax.set_xlim(int(xmin-1), int(xmax+1))\n",
    "\t\tys = []\n",
    "\t\txticks = []\n",
    "\t\tstep_size = 0.25\n",
    "\t\tfor xtick in np.arange(np.floor(xmin), np.ceil(xmax), step_size):\n",
    "\t\t\txticks.append(xtick)\n",
    "\t\t\tys.append(len([x for x in xs if (xtick-step_size/2<x<=xtick+step_size/2)]))\n",
    "\t\tax.bar(xticks, ys, color=color, alpha=0.3, width=step_size)\n",
    "\t\t\n",
    "\tdef visualize(self, xs, labels, colors, parties, title, xlabel, path='plot.png'):\n",
    "\t\tunique_parties = set(parties)\n",
    "\t\tmax_x = max(xs)\n",
    "\t\tmin_x = min(xs)\n",
    "\t\tfig, axs = plt.subplots(5,2, figsize=(10,20))\n",
    "\t\taxs[0,0].scatter(xs, [PARTY_TO_IDX[party] for party in parties], c =colors, alpha=0.3)\n",
    "\t\taxs[0,0].set_title(title)\n",
    "\t\taxs[0,0].set_xlabel(xlabel)\n",
    "\t\taxs[0,0].set_xlim(int(min_x-1), int(max_x+1))\n",
    "\t\ty_ticks = axs[0,0].get_yticks()\n",
    "\t\ty_ticks_text = []\n",
    "\t\tfor y_tick in y_ticks:\n",
    "\t\t\tif y_tick in IDX_TO_PARTY.keys():\n",
    "\t\t\t\ty_ticks_text.append(IDX_TO_PARTY[y_tick])\n",
    "\t\t\telse:\n",
    "\t\t\t\ty_ticks_text.append('')\n",
    "\t\taxs[0,0].set_yticklabels(y_ticks_text)\n",
    "\t\t#Flatten axis\n",
    "\t\taxs = axs.reshape(-1)\n",
    "\t\tfor idx, party in enumerate(unique_parties):\n",
    "\t\t\tself.plot_grouped_bar_chart(axs[idx+1],\n",
    "\t\t\t\t\t\t\t   xs=[x for x, p in zip(xs, parties) if p == party],\n",
    "\t\t\t\t\t\t\t   party=party,\n",
    "\t\t\t\t\t\t\t   xmax=max_x,\n",
    "\t\t\t\t\t\t\t   xmin=min_x,\n",
    "\t\t\t\t\t\t\t   title=party,\n",
    "\t\t\t\t\t\t\t   xlabel='',\n",
    "\t\t\t\t\t\t\t   ylabel='',\n",
    "\t\t\t\t\t\t\t   color=PARTY_TO_COLOR[party])\n",
    "\n",
    "\t\tfig.tight_layout()\n",
    "\t\tplt.savefig(path)\n",
    "\t\tplt.clf()\n",
    "\t\tplt.cla()\n",
    "\t\tplt.close()\n",
    "\t\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Collapse all the other vectors onto this axis by projecting them onto the axis\n",
    "\n",
    "## 5. Create a scalar measurement for how far each politician is from the two reference points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualizing 防衛\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_27416/3229737788.py:77: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  axs[0,0].set_yticklabels(y_ticks_text)\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 61.16it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualizing 少子化\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 57.00it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualizing LGBTQ\n",
      "志位和夫 not in ['新藤義孝', '柴山昌彦', '岸田文雄', '菅義偉', '林芳正', '遠藤利明', '後藤茂之', '牧島かれん', '山下貴司', '加藤勝信', '稲田朋美', '齋藤健', '上川陽子', '加藤鮎子', '山田賢司', '永岡桂子', '小倉將信', 'あかま二郎', '松野博一', '鈴木俊一', '野田聖子', '田畑裕明', '丹羽秀樹', '根本匠', '坂本哲志', '橋本岳', '高市早苗', '西村康稔', '津島淳', '井出庸生', '松本剛明', '若宮健嗣', '斎藤アレックス', '日下正喜', '河西宏一', '國重徹', '吉田久美子', '鰐淵洋子', '泉健太', '枝野幸男', '小宮山泰子', '柚木道義', '道下大樹', '吉田はるみ', '中谷一馬', '西村智奈美', '阿部司', '岩谷良平', '吉田とも代', '田村貴昭', '宮本岳志', '宮本徹', '本村伸子']\n",
      "Visualizing 原発\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 45.00it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualizing 気候変動\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 66.35it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualizing 経済対策\n",
      "'for'\n"
     ]
    }
   ],
   "source": [
    "vo = VectorOperator()\n",
    "psv = PoliticalStanceVisualizer()\n",
    "for topic_config in EXPERIMENT_CONFIG:\n",
    "\ttry:\n",
    "\t\ttopic = topic_config['topic_name']\n",
    "\t\tprint(f'Visualizing {topic}')\n",
    "\t\tfor_repr_name = topic_config[\"repr_references\"][\"for\"]\n",
    "\t\tagainst_repr_name = topic_config[\"repr_references\"][\"against\"]\n",
    "\t\tgen_for_sentence = topic_config[\"generated_references\"][\"for\"]\n",
    "\t\tgen_against_sentence = topic_config[\"generated_references\"][\"against\"]\n",
    "\t\tprojections, reprs = vo.collapse_vectors_onto_two_ref_reprs(summary_hdf5_path = os.path.join(DATA_REPR_SPEECHES_DIR, f'{topic}.hdf5'),\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\ttopic=topic,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tref_repr1=for_repr_name,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tref_repr2=against_repr_name)\n",
    "\t\tparties = [vo.embedding_handler.repr2party[repr] for repr in reprs]\n",
    "\n",
    "\t\tpsv.visualize(xs=projections,\n",
    "\t\t\t\t\tlabels=reprs,\n",
    "\t\t\t\t\tcolors=[PARTY_TO_COLOR[party] for party in parties],\n",
    "\t\t\t\t\tparties=parties,\n",
    "\t\t\t\t\ttitle=topic,\n",
    "\t\t\t\t\txlabel=f'{for_repr_name} -> {against_repr_name}',\n",
    "\t\t\t\t\tpath=os.path.join(DATA_DIR, \"plots\" ,f'{topic}.png')\n",
    "\t\t\t\t\t)\n",
    "\t\tprojections, reprs = vo.collapse_vectors_onto_two_genenerated_strings(summary_hdf5_path = os.path.join(DATA_REPR_SPEECHES_DIR, f'{topic}.hdf5'),\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\ttopic=topic,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tstring1=gen_for_sentence,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tstring2=gen_against_sentence)\n",
    "\t\tparties = [vo.embedding_handler.repr2party[repr] for repr in reprs]\n",
    "\n",
    "\t\tpsv.visualize(xs=projections,\n",
    "\t\t\t\t\tlabels=reprs,\n",
    "\t\t\t\t\tcolors=[PARTY_TO_COLOR[party] for party in parties],\n",
    "\t\t\t\t\tparties=parties,\n",
    "\t\t\t\t\ttitle=topic,\n",
    "\t\t\t\t\txlabel=f'for -> against',\n",
    "\t\t\t\t\tpath=os.path.join(DATA_DIR, \"plots\" ,f'{topic}_gen.png')\n",
    "\t\t\t\t\t)\n",
    "\n",
    "\texcept Exception as e:\n",
    "\t\tprint(e) \n",
    "\t\tcontinue\n",
    "\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modelling on Politicians "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Group politicians based on position on the axis  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definitions \n",
    "    # group is the part of the axes \n",
    "    # party is the political party\n",
    "    # rep is the representative (i.e., politician name)\n",
    "\n",
    "# a. Divide the axis into n parts\n",
    "n = 3\n",
    "\n",
    "axis_range = np.max(projections) - np.min(projections) # TODO: possibly re-adjust the range to reflect the range in the viz?\n",
    "portion_size = axis_range / n\n",
    "dividing_points = [np.min(projections) + i * portion_size for i in range(1, 3)]\n",
    "\n",
    "# b. Group politicians based on their position on the axis \n",
    "groups = np.digitize(projections, dividing_points)\n",
    "\n",
    "# c. Add politician and party to dict # group 1, 2, 3 (from left to right)\n",
    "group_dict = {group_num: {'politicians': [], 'parties': set()} for group_num in range(1, 4)}\n",
    "\n",
    "for rep, group, party in zip(reprs, groups, parties):\n",
    "    group_dict[group + 1]['politicians'].append(rep) # added +1 because of zero indexing \n",
    "    group_dict[group + 1]['parties'].add(party)\n",
    "\n",
    "group_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Get the opinion sentences of each group on the axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "\n",
    "topic = '少子化'\n",
    "\n",
    "def text_preprocessing(texts):\n",
    "    processed_texts = []\n",
    "    for text in texts:\n",
    "        # remove specific characters using re.sub\n",
    "        text = re.sub('[、\\r\\n\\u3000]', '', text)\n",
    "        processed_texts.append(text)\n",
    "    return processed_texts\n",
    "\n",
    "group_topics = {}\n",
    "group_dict_2 = group_dict.copy() # copy because cannot change size while we iterate\n",
    "\n",
    "for group_num, group_info in group_dict.items():\n",
    "    opinion_sentences = []\n",
    "    \n",
    "    for politician in group_info['politicians']:\n",
    "        embeddings, dates, opinions = eh.get_embeddings_for_repr_for_topic(politician, topic)\n",
    "        opinion_sentences.extend(opinions)\n",
    "\n",
    "    #preprocess\n",
    "    preprocessed_opinions = text_preprocessing(opinion_sentences)\n",
    "    group_dict_2[group_num]['opinions_text'] = preprocessed_opinions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Topic Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: fixing stopwords to get rid of unwanted common ideas\n",
    "\n",
    "import MeCab\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "mecab = MeCab.Tagger('-Owakati')\n",
    "\n",
    "def tokenize_jp(text):\n",
    "    text = re.sub('[、\\r\\n\\u3000]', '', text)\n",
    "\n",
    "    # Read stopwords from file\n",
    "    with open('stopwords-ja.txt', encoding='utf-8') as f:\n",
    "        stopwords = set(f.read().split())\n",
    "    \n",
    "    words = MeCab.Tagger(\"-Owakati\").parse(text).split()\n",
    "    \n",
    "    # Remove stopwords from words\n",
    "    words = [w for w in words if w not in stopwords] # TODO: find a comprehensive list of jp stopwords to make sure we get more sensical topic outputs\n",
    "    \n",
    "    return words\n",
    "\n",
    "## test tockenizer \n",
    "# test = \"今後も言語モデルの発展に伴って付け替え可能。文書のクラスタリングUMAPで次元削減した後、 HDBSCANでクラスタリング。次元削減手法やクラスタリング手法も付け替え可能。\"\n",
    "# tokenize_jp(test)\n",
    "\n",
    "def apply_bertopic(group_dict: dict, group_num: int, nr_topics: int): # \"cl-tohoku/bert-base-japanese-v3\" has sentence embeddings as st\n",
    "    MODEL_NAME = st #\"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\"  # smaller model: \"paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "    vectorizer_model = CountVectorizer(tokenizer=tokenize_jp)\n",
    "    model = BERTopic(verbose=\"True\", embedding_model=MODEL_NAME, nr_topics=nr_topics, vectorizer_model=vectorizer_model) \n",
    "    \n",
    "    topics, probs  = model.fit_transform(group_dict[group_num]['opinions_text'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "group_1 = apply_bertopic(group_dict_2, 1, 10)\n",
    "# group_2 = apply_bertopic(2, 10)\n",
    "# group_3 = apply_bertopic(3, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(group_1.get_topic_info())\n",
    "# display(group_2.get_topic_info())\n",
    "# display(group_3.get_topic_info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_1.visualize_barchart(n_words=10)\n",
    "# group_1.visualize_topics()\n",
    "# group_1.get_topics()\n",
    "# group_1.get_topic(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA (would need fixing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re \n",
    "# from sklearn.decomposition import LatentDirichletAllocation\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "# from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# topic = '少子化'\n",
    "\n",
    "# num_topics = 5 \n",
    "\n",
    "\n",
    "# mecab = MeCab.Tagger('-Owakati')\n",
    "# def tokenize_jp(text):\n",
    "#     text = re.sub('[、\\r\\n\\u3000]', '', text)\n",
    "\n",
    "#     # Read stopwords from file\n",
    "#     with open('stopwords-ja.txt', encoding='utf-8') as f:\n",
    "#         stopwords = set(f.read().split())\n",
    "    \n",
    "#     words = MeCab.Tagger(\"-Owakati\").parse(text).split()\n",
    "    \n",
    "#     # Remove stopwords from words\n",
    "#     words = [w for w in words if w not in stopwords]\n",
    "    \n",
    "#     return words\n",
    "\n",
    "\n",
    "# lda_model = make_pipeline(CountVectorizer(analyzer=tokenize_jp), LatentDirichletAllocation(n_components=num_topics, random_state=42))\n",
    "\n",
    "# group_topics = {}\n",
    "\n",
    "# # retrieve opinion sentences\n",
    "# for group_num, group_info in group_dict.items():\n",
    "#     opinion_sentences = []\n",
    "    \n",
    "#     for politician in group_info['politicians']:\n",
    "#         embeddings, dates, opinions = eh.get_embeddings_for_repr_for_topic(politician, '少子化')\n",
    "#         opinion_sentences.extend(opinions)\n",
    "\n",
    "#     # fit LDA on the opinion sentences\n",
    "#     lda_model.fit(opinion_sentences)\n",
    "    \n",
    "#     # get topics and their associated words\n",
    "#     feature_names = lda_model.named_steps['countvectorizer'].get_feature_names_out()\n",
    "#     topics = []\n",
    "#     for topic_idx, topic in enumerate(lda_model.named_steps['latentdirichletallocation'].components_):\n",
    "#         topic_words = [feature_names[i] for i in topic.argsort()[:-10 - 1:-1]]\n",
    "#         topics.append(topic_words)\n",
    "    \n",
    "#     group_topics[group_num] = topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
