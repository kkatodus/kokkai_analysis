{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About the code\n",
    "This code is going to be the main to collect and create analysis output to answer the question \"Quantifying political stance of Japanese Diet members regards specific political topics through the use of LLMs and statistical methods.\" (Tentative) \n",
    "\n",
    "## Part 1: Procedure for measuring embeddings\n",
    "1. Create embeddings for each opinion-based sentence in regards to different topics and store it in a retrievable manner.\n",
    "2. Create one single \"opinion-embedding\" for each politician and store it in a retrievable manner.\n",
    "3. Create a stance axis vector by either generating two reference points or picking two points \"opinion-embeddings\" from the data\n",
    "4. Collapse all the other vectors onto this axis by projecting them onto the axis\n",
    "5. Create a scalar measurement for how far each politician is from the two reference points\n",
    "6. Use UMAP dim reduction to see the positions of the politicians as well as the generated/selected reference points\n",
    "\n",
    "## Part 2: Creating groups of politicians based on where their embeddings lie on an axis \n",
    "This is to get an idea of the ideas mentioned by politicians in a axis group within each topic using BERTopic. \n",
    "1. Divide politicians into n groups based on where they lie on the axis.\n",
    "2. Extract opinion sentences of each group made. \n",
    "3. Run each group through topic modelling techniques.\n",
    "\n",
    "### Notes\n",
    "- Data is stored under `data/data_repr` directory\n",
    "- We will attempt the procedure with different models to seek the best output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "DATA_DIR:  /root/projects/kokkai_analysis/data_prepping/params/../data\n",
      "DATA_REPR_SPEECHES_DIR:  /root/projects/kokkai_analysis/data_prepping/params/../data/data_repr_lower\n",
      "PARTIES:  ['自民', '国民', '公明', '立憲', 'れ新', '維新', '無', '共産', '有志']\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from sentence_transformers import models, SentenceTransformer\n",
    "import h5py\n",
    "import umap\n",
    "import numpy as np\n",
    "import torch\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from params.paths import ROOT_DIR\n",
    "import japanize_matplotlib \n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "from file_handling.file_read_writer import read_json, write_json, create_dir, write_file, read_hdf5_file, read_txt_file\n",
    "import random as rnd\n",
    "\n",
    "VERBOSE = True\n",
    "\n",
    "\n",
    "#Data Dir\n",
    "DATA_DIR = os.path.join(ROOT_DIR, 'data')\n",
    "DATA_REPR_SPEECHES_DIR = os.path.join(DATA_DIR, 'data_repr_lower')\n",
    "#Resources\n",
    "RESOURCE_DIR = os.path.join(ROOT_DIR, 'resource')\n",
    "#Results\n",
    "RESULTS_DIR = os.path.join(ROOT_DIR, 'results')\n",
    "TODAYS_RESULTS = os.path.join(RESULTS_DIR, datetime.now().strftime('%Y%m%d'))\n",
    "#Logger\n",
    "log_dir = os.path.join(TODAYS_RESULTS, 'logs')\n",
    "create_dir(log_dir)\n",
    "logging.basicConfig(filename=os.path.join(log_dir, 'quantify_politician_stance.log'), filemode='w', format='%(asctime)s - %(message)s')\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "create_dir(RESULTS_DIR)\n",
    "create_dir(TODAYS_RESULTS)\n",
    "#Plots\n",
    "TODAYS_PLOTS = os.path.join(TODAYS_RESULTS, 'plots')\n",
    "create_dir(TODAYS_PLOTS)\n",
    "#Configs\n",
    "EXPERIMENT_CONFIG_PATH = os.path.join(RESOURCE_DIR, 'experiment_config.json')\n",
    "EXPERIMENT_CONFIG = read_json(EXPERIMENT_CONFIG_PATH)\n",
    "#Other data\n",
    "PARTIES = [party for party in os.listdir(DATA_REPR_SPEECHES_DIR) if not '.' in party]\n",
    "PARTY_TO_COLOR = {\n",
    "\t'自民': 'black',\n",
    "\t'国民': 'blue',\n",
    "\t'立憲': 'orange',\n",
    "\t'公明': 'lightblue',\n",
    "\t'共産': 'red',\n",
    "\t'維新': 'gold',\n",
    "\t'れ新': 'green',\n",
    "\t'無': 'purple',\n",
    "\t'有志': 'grey'\n",
    "}\n",
    "TOPICJP_TO_TOPICEN = {\n",
    "\t'防衛': 'Defence',\n",
    "\t'原発': 'Nuclear Power',\n",
    "\t'経済': 'Economy',\n",
    "\t'気候変動': 'Climate Change',\n",
    "\t'少子化': 'Declining Birthrate',\n",
    "}\n",
    "PARTYJP_TO_PARTYEN = {\n",
    "\t'自民': 'LDP',\n",
    "\t'国民': 'NDP',\n",
    "\t'立憲': 'CDP',\n",
    "\t'公明': 'Komeito',\n",
    "\t'共産': 'JCP',\n",
    "\t'維新': 'JRP',\n",
    "\t'れ新': 'Reiwa',\n",
    "\t'無': 'None',\n",
    "\t'有志': 'Independents'\n",
    "\n",
    "}\n",
    "IGNORE_PARTIES = ['無', '有志', 'れ新']\n",
    "PARTY_TO_IDX = {party: idx for idx, party in enumerate(PARTIES)}\n",
    "IDX_TO_PARTY = {idx: party for idx, party in enumerate(PARTIES)}\n",
    "if len(PARTIES) != len(PARTY_TO_COLOR):\n",
    "\traise ValueError('PARTIES and PARTY_TO_COLOR must have the same length.')\n",
    "MODEL_NAME = \"cl-tohoku/bert-base-japanese-v3\"\n",
    "print('-----------------------------------')\n",
    "print('DATA_DIR: ', DATA_DIR)\n",
    "print('DATA_REPR_SPEECHES_DIR: ', DATA_REPR_SPEECHES_DIR)\n",
    "print('PARTIES: ', PARTIES)\n",
    "print('-----------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['特急', 'は', 'く', 'た', 'か']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 48.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape torch.Size([768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# adapted from: https://osima.jp/posts/sentence-bert/\n",
    "def fix_seed(seed=42):\n",
    "\tnp.random.seed(seed)\n",
    "\ttorch.manual_seed(seed)\n",
    "\ttorch.cuda.manual_seed(seed)\n",
    "\ttorch.backends.cudnn.deterministic = True\n",
    "\n",
    "fix_seed()\n",
    "\n",
    "sentence_transformer = models.Transformer(MODEL_NAME)\n",
    "\n",
    "pooling = models.Pooling(\n",
    "    sentence_transformer.get_word_embedding_dimension(),\n",
    "    pooling_mode_mean_tokens=False,\n",
    "    pooling_mode_cls_token=True,\n",
    "    pooling_mode_max_tokens=False)\n",
    "\n",
    "st = SentenceTransformer(modules=[sentence_transformer, pooling])\n",
    "tk = st.tokenizer\n",
    "print(tk.tokenize('特急はくたか'))\n",
    "gen_for_embedding = st.encode('これとかあれとか', convert_to_tensor=True, show_progress_bar=True)\n",
    "print('shape', gen_for_embedding.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Creating the embeddings for each opinion-based sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_speeches(speeches):\n",
    "\tencoded_opinions = st.encode(speeches, convert_to_tensor=True, show_progress_bar=True)\n",
    "\treturn encoded_opinions\n",
    "\n",
    "def read_opinion_sentences_and_dates(file_path):\n",
    "\tlogger.message(f'Reading {file_path}')\n",
    "\ttarget_dict = read_json(file_path)\n",
    "\tif not target_dict:\n",
    "\t\treturn [], []\n",
    "\topinion_sentences = []\n",
    "\tdates = []\n",
    "\tfor speech in target_dict['speeches']:\n",
    "\t\tdate = [speech['date'] for _ in range(len(speech['extracted_opinions']))]\n",
    "\t\topinions = speech['extracted_opinions']\n",
    "\t\topinion_sentences.extend(opinions)\n",
    "\t\tdates.extend(date)\n",
    "\t\n",
    "\treturn opinion_sentences, dates\n",
    "\n",
    "def iterate_topics_for_repr(repr_path, topics=[]):\n",
    "\tfor topic in os.listdir(repr_path):\n",
    "\t\tif topic not in topics and topics:\n",
    "\t\t\tcontinue\n",
    "\t\tlogger.message(f'Working on {topic}')\n",
    "\t\ttopic_path = os.path.join(repr_path, topic)\n",
    "\t\tif os.path.exists(os.path.join(topic_path, 'embeddings.hdf5')):\n",
    "\t\t\tlogger.message(f'Embeddings already exist for {topic} in {repr_path}')\n",
    "\t\t\tcontinue\n",
    "\t\tfile_path = os.path.join(topic_path, 'opinions.json')\n",
    "\t\ttopic_opinions, topic_dates = read_opinion_sentences_and_dates(file_path)\n",
    "\t\tif not topic_opinions:\n",
    "\t\t\tlogger.message(f'No opinions found for {topic} in {repr_path}')\n",
    "\t\t\tcontinue\n",
    "\t\tembeddings = embed_speeches(topic_opinions)\n",
    "\t\tembeddings = [embedding.cpu() for embedding in embeddings]\n",
    "\t\tlogger.message(f'Number of dates {len(topic_dates)}\\nNumber of opinions {len(topic_opinions)} \\nNumber of embeddings {len(embeddings)}')\n",
    "\t\tembeddings = torch.stack(embeddings)\n",
    "\t\twith h5py.File(os.path.join(topic_path, 'embeddings.hdf5'), 'w') as f:\n",
    "\t\t\tf.create_dataset('embeddings', data=embeddings)\n",
    "\t\t\tf.create_dataset('dates', data=topic_dates, dtype=h5py.string_dtype(encoding='utf-8'))\n",
    "\t\t\tf.create_dataset('opinions', data=topic_opinions, dtype=h5py.string_dtype(encoding='utf-8'))\n",
    "\n",
    "TOPICS_TO_CREATE_EMBEDDINGS = ['LGBTQ', '原発', '少子化', '気候変動', '経済対策', '防衛']\n",
    "for party in PARTIES:\n",
    "\tparty_path = os.path.join(DATA_REPR_SPEECHES_DIR, party)\n",
    "\trepr_names = os.listdir(party_path)\n",
    "\tfor repr_name in repr_names:\n",
    "\t\tlogger.message(f'{party} ----- {repr_name}')\n",
    "\t\trepr_path = os.path.join(party_path, repr_name)\n",
    "\t\titerate_topics_for_repr(repr_path, topics=TOPICS_TO_CREATE_EMBEDDINGS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create one single \"opinion-embedding\" for each politician and store it in a retrievable manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingHandler:\n",
    "\tdef __init__(self, data_dir=DATA_DIR, speeches_dir=DATA_REPR_SPEECHES_DIR, parties=PARTIES):\n",
    "\t\tself.data_dir = data_dir\n",
    "\t\tself.speeches_dir = speeches_dir\n",
    "\t\tself.parties = parties\n",
    "\t\tself.reprs = []\n",
    "\t\tself.repr2party = {}\n",
    "\t\tfor party in self.parties:\n",
    "\t\t\treprs = os.listdir(os.path.join(self.speeches_dir, party))\n",
    "\t\t\tself.reprs.extend(reprs)\n",
    "\t\t\tfor repr in reprs:\n",
    "\t\t\t\tself.repr2party[repr] = party\n",
    "\n",
    "\tdef check_party_exists(self, party):\n",
    "\t\tif party not in self.parties:\n",
    "\t\t\traise ValueError(f'{party} not in {self.parties}')\n",
    "\t\t\n",
    "\tdef get_reprs_for_party(self, party):\n",
    "\t\tself.check_party_exists(party)\n",
    "\t\treturn os.listdir(os.path.join(self.speeches_dir, party))\n",
    "\n",
    "\tdef get_topics_for_repr(self, party, repr_name):\n",
    "\t\tself.check_party_exists(party)\n",
    "\t\treturn os.listdir(os.path.join(self.speeches_dir, party, repr_name))\n",
    "\n",
    "\tdef get_embeddings_for_repr_for_topic(self,repr_name, topic, party=None):\n",
    "\t\tif party is None:\n",
    "\t\t\tparty = self.repr2party[repr_name]\n",
    "\t\tpath = os.path.join(self.speeches_dir, party, repr_name, topic, 'embeddings.hdf5')\n",
    "\t\tif not os.path.exists(path):\n",
    "\t\t\traise ValueError(f'{path} does not exist')\n",
    "\t\t\n",
    "\t\twith h5py.File(path, 'r') as f:\n",
    "\t\t\tembeddings = f['embeddings'][:]\n",
    "\t\t\tdates = [date.decode('utf-8') for date in f['dates'][:]]\n",
    "\t\t\topinions = [opinion.decode('utf-8') for opinion in f['opinions'][:]]\n",
    "\t\t\treturn embeddings, dates, opinions\n",
    "\t\t\n",
    "\tdef get_repr_speech_freq(self,repr_name, topic, party=None):\n",
    "\t\tif party is None:\n",
    "\t\t\tparty = self.repr2party[repr_name]\n",
    "\t\tpath = os.path.join(self.speeches_dir, party, repr_name, topic, 'opinions.json')\n",
    "\t\tif not os.path.exists(path):\n",
    "\t\t\traise ValueError(f'{path} does not exist')\n",
    "\t\topinions_json = read_json(path)\n",
    "\t\tif not opinions_json:\n",
    "\t\t\treturn 0\n",
    "\t\treturn len(opinions_json['speeches'])\n",
    "\t\t\n",
    "\tdef get_average_embedding_for_repr_for_topic(self, repr_name, topic, party=None):\n",
    "\t\tif party is None:\n",
    "\t\t\tparty = self.repr2party[repr_name]\n",
    "\t\tembeddings, _, _= self.get_embeddings_for_repr_for_topic(repr_name, topic, party=party)\n",
    "\t\treturn np.mean(embeddings, axis=0)\n",
    "\t\n",
    "\tdef create_summary_hdf5_file_for_average_embeddings(self, path, topic, reprs=None, ignore_repr_speech_freq_threshold=4, start_date=None, end_date=None):\n",
    "\t\tif reprs is None:\n",
    "\t\t\treprs = self.reprs\n",
    "\t\tif start_date is None:\n",
    "\t\t\tstart_date = '0000-01-01'\n",
    "\t\tif end_date is None:\n",
    "\t\t\tend_date = '9999-01-01'\n",
    "\t\tembeddings = []\n",
    "\t\treprs_with_embeddings = []\n",
    "\t\tfor repr in reprs:\n",
    "\t\t\t\tembedding_path = os.path.join(self.speeches_dir, self.repr2party[repr], repr, topic, 'embeddings.hdf5')\n",
    "\t\t\t\tif not os.path.exists(embedding_path):\n",
    "\t\t\t\t\tlogger.message(f'No embeddings for {repr} for {topic}')\n",
    "\t\t\t\t\tcontinue\n",
    "\t\t\t\tif self.get_repr_speech_freq(repr, topic) < ignore_repr_speech_freq_threshold:\n",
    "\t\t\t\t\tlogger.message(f'Ignoring {repr} because it has less than {ignore_repr_speech_freq_threshold} speeches')\n",
    "\t\t\t\t\tcontinue\n",
    "\t\t\t\taverage_embedding = self.get_average_embedding_for_repr_for_topic(repr, topic)\n",
    "\t\t\t\tembeddings.append(average_embedding)\n",
    "\t\t\t\treprs_with_embeddings.append(repr)\n",
    "\n",
    "\t\tembeddings = np.array(embeddings)\n",
    "\t\twith h5py.File(path, 'w') as f:\n",
    "\t\t\tf.create_dataset('embeddings', data=embeddings)\n",
    "\t\t\tf.create_dataset('reprs', data=reprs_with_embeddings, dtype=h5py.string_dtype(encoding='utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating summary hdf5 file for average embeddings for some topics for reprs where the data is available\n",
    "eh = EmbeddingHandler()\n",
    "TOPICS_TO_CREATE_AVERAGE_EMBEDDINGS = ['LGBTQ', '原発', '少子化', '気候変動', '経済対策', '防衛']\n",
    "for topic in TOPICS_TO_CREATE_AVERAGE_EMBEDDINGS:\n",
    "\tprint(f'Creating summary hdf5 file for all embeddings for one topic {topic}')\n",
    "\teh.create_summary_hdf5_file_for_average_embeddings(path = os.path.join(DATA_REPR_SPEECHES_DIR, f'{topic}.hdf5'),\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\ttopic = topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(eh.reprs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create a stance axis vector by either generating two reference points or picking two points \"opinion-embeddings\" from the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "class VectorOperator:\n",
    "\tdef __init__(self):\n",
    "\t\tself.embedding_handler = EmbeddingHandler()\n",
    "\t\n",
    "\tdef project_vector(vector, onto_vector):\n",
    "\t\tnormalized_onto_vector = onto_vector / np.linalg.norm(onto_vector)\n",
    "\t\tscaling = np.dot(vector, normalized_onto_vector)\n",
    "\t\tprojection = scaling * normalized_onto_vector\n",
    "\t\treturn projection, scaling\n",
    "\t\n",
    "\tdef get_embeddings_and_reprs(self, summary_hdf5_path):\n",
    "\t\thdf5_dict = read_hdf5_file(summary_hdf5_path)\n",
    "\t\tembeddings = hdf5_dict['embeddings'][:]\n",
    "\t\treprs = [repr.decode('utf-8') for repr in hdf5_dict['reprs'][:]]\n",
    "\t\treturn embeddings, reprs\n",
    "\t\n",
    "\tdef reduce_dimensions_umap(self, embeddings, n_components=2):\n",
    "\t\tumap_embeddings = umap.UMAP(n_components=n_components, verbose=True, n_neighbors=30).fit_transform(embeddings)\n",
    "\t\treturn umap_embeddings\n",
    "\n",
    "\tdef collapse_vectors_onto_two_ref_reprs(self, summary_hdf5_path, topic, ref_repr1, ref_repr2):\n",
    "\t\tembeddings, reprs = self.get_embeddings_and_reprs(summary_hdf5_path)\n",
    "\t\tif ref_repr1 not in reprs:\n",
    "\t\t\traise ValueError(f'{ref_repr1} not in {reprs}')\n",
    "\t\tif ref_repr2 not in reprs:\n",
    "\t\t\traise ValueError(f'{ref_repr2} not in {reprs}')\n",
    "\t\tref1_embedding = self.embedding_handler.get_average_embedding_for_repr_for_topic(ref_repr1, topic)\n",
    "\t\tref2_embedding = self.embedding_handler.get_average_embedding_for_repr_for_topic(ref_repr2, topic)\n",
    "\t\tref2_to_ref1 = ref1_embedding - ref2_embedding\n",
    "\t\tprojections = embeddings @ ref2_to_ref1\n",
    "\t\tprojections = projections / np.linalg.norm(ref2_to_ref1)\n",
    "\t\treturn projections, reprs\n",
    "\n",
    "\tdef collapse_vectors_onto_two_genenerated_strings(self, summary_hdf5_path, topic, string1, string2):\n",
    "\t\tembeddings, reprs = self.get_embeddings_and_reprs(summary_hdf5_path)\n",
    "\t\tstring1_embedding = st.encode(string1, convert_to_tensor=True, show_progress_bar=True)\n",
    "\t\tstring2_embedding = st.encode(string2, convert_to_tensor=True, show_progress_bar=True)\n",
    "\t\tstring1_embedding = string1_embedding.cpu().numpy()\n",
    "\t\tstring1_embedding = np.mean(string1_embedding, axis=0)\n",
    "\t\tstring2_embedding = string2_embedding.cpu().numpy()\n",
    "\t\tstring2_embedding = np.mean(string2_embedding, axis=0)\n",
    "\t\tstring1_to_string2 = string1_embedding - string2_embedding\n",
    "\t\tprojections = embeddings @ string1_to_string2\n",
    "\t\tprojections = projections / np.linalg.norm(string1_to_string2)\n",
    "\t\treturn projections, reprs\n",
    "\t\n",
    "class PoliticalStanceVisualizer:\n",
    "\tdef __init__(self):\n",
    "\t\tpass\n",
    "\n",
    "\tdef visualize_red_dimension(self, red_dims, reprs, topic, parties, colors, for_repr_idx=0, against_repr_idx=0, path='plot.png', title='', show_repr_names=False):\n",
    "\t\tfig, ax = plt.subplots(figsize=(10,10))\n",
    "\t\tax.scatter(red_dims[:,0], red_dims[:, 1], c=colors, alpha=0.3, label=parties)\n",
    "\t\tlegend_items = [Line2D([0], [0], marker='o', color='w', label=PARTYJP_TO_PARTYEN[party], markerfacecolor=color, markersize=10, alpha=0.3) for party, color in PARTY_TO_COLOR.items()]\n",
    "\t\tax.legend(handles=legend_items)\n",
    "\t\tax.scatter(red_dims[for_repr_idx, 0], red_dims[for_repr_idx, 1], edgecolors='blue', facecolors='none', s=200)\n",
    "\t\tax.scatter(red_dims[against_repr_idx, 0], red_dims[against_repr_idx, 1], edgecolors='red', facecolors='none', s=200)\n",
    "\t\tax.plot([red_dims[for_repr_idx, 0], red_dims[against_repr_idx, 0]], [red_dims[for_repr_idx, 1], red_dims[against_repr_idx, 1]], c='black')\n",
    "\t\tif show_repr_names:\n",
    "\t\t\tfor idx, repr in enumerate(reprs):\n",
    "\t\t\t\tax.annotate(repr, (red_dims[idx, 0], red_dims[idx, 1]), fontsize=7)\n",
    "\t\tax.set_title(title)\n",
    "\t\tax.set_xlabel('Red dimension 1')\n",
    "\t\tax.set_ylabel('Red dimension 2')\n",
    "\t\tfig.tight_layout()\n",
    "\t\tplt.savefig(path)\n",
    "\t\tplt.clf()\n",
    "\t\tplt.cla()\n",
    "\t\tplt.close()\n",
    "\n",
    "\tdef plot_grouped_bar_chart(self, ax, xs, party, xmax, xmin, title, xlabel='', ylabel=\"\", color=\"blue\"):\n",
    "\t\tax.set_title(title)\n",
    "\t\tax.set_xlabel(xlabel)\n",
    "\t\tax.set_ylabel(ylabel)\n",
    "\t\tax.set_xlim(int(xmin-1), int(xmax+1))\n",
    "\t\tys = []\n",
    "\t\txticks = []\n",
    "\t\tstep_size = 0.25\n",
    "\t\tfor xtick in np.arange(np.floor(xmin), np.ceil(xmax), step_size):\n",
    "\t\t\txticks.append(xtick)\n",
    "\t\t\tys.append(len([x for x in xs if (xtick-step_size/2<x<=xtick+step_size/2)]))\n",
    "\t\tax.bar(xticks, ys, color=color, alpha=0.3, width=step_size)\n",
    "\t\t\n",
    "\tdef save_2d_plot(self, red_dims, reprs, colors, parties, filename, out_dir, for_repr_name, against_repr_name, for_repr_idx, against_repr_idx):\n",
    "\t\t#save 2d hdf5 file in dir\n",
    "\t\t#save json file in dir\n",
    "\t\thdf_5_path = os.path.join(out_dir, filename+'.hdf5')\n",
    "\t\twith h5py.File(hdf_5_path, 'w') as f:\n",
    "\t\t\tf.create_dataset('red_dims', data=red_dims)\n",
    "\t\t\tf.create_dataset('reprs', data=reprs, dtype=h5py.string_dtype(encoding='utf-8'))\n",
    "\t\t\tf.create_dataset('parties', data=parties, dtype=h5py.string_dtype(encoding='utf-8'))\n",
    "\t\t\tf.create_dataset('colors', data=colors, dtype=h5py.string_dtype(encoding='utf-8'))\n",
    "\t\tout_dict = {\"data\":[]}\n",
    "\t\tfor idx, (red_dim, repr, party, color) in enumerate(zip(red_dims, reprs, parties, colors)):\n",
    "\t\t\tout_dict['data'].append({\n",
    "\t\t\t\t'idx': idx,\n",
    "\t\t\t\t'x': str(red_dim[0]),\n",
    "\t\t\t\t'y': str(red_dim[1]),\n",
    "\t\t\t\t'repr': repr,\n",
    "\t\t\t\t'party': party,\n",
    "\t\t\t\t'color': color,\n",
    "\t\t\t\t'ref_point': 'for' if repr == for_repr_name else 'against' if repr == against_repr_name else 'none'\n",
    "\t\t\t})\n",
    "\t\tjson_path = os.path.join(out_dir, filename+'.json')\n",
    "\t\twrite_json(path=json_path, dict_obj=out_dict)\n",
    "\n",
    "\tdef save_1d_plot(self, xs, reprs, colors, parties, filename, out_dir):\n",
    "\t\t#save 1d hdf5 file in dir\n",
    "\t\t#save json file in dir\n",
    "\t\thdf_5_path = os.path.join(out_dir, filename+'.hdf5')\n",
    "\t\twith h5py.File(hdf_5_path, 'w') as f:\n",
    "\t\t\tf.create_dataset('projections', data=xs)\n",
    "\t\t\tf.create_dataset('reprs', data=reprs, dtype=h5py.string_dtype(encoding='utf-8'))\n",
    "\t\t\tf.create_dataset('parties', data=parties, dtype=h5py.string_dtype(encoding='utf-8'))\n",
    "\t\t\tf.create_dataset('colors', data=colors, dtype=h5py.string_dtype(encoding='utf-8'))\n",
    "\t\tout_dict = {\"data\":[]}\n",
    "\t\tfor idx, (x, repr, party, color) in enumerate(zip(xs, reprs, parties, colors)):\n",
    "\t\t\tout_dict['data'].append({\n",
    "\t\t\t\t'idx': idx,\n",
    "\t\t\t\t'y' : str(idx),\n",
    "\t\t\t\t'x': str(x),\n",
    "\t\t\t\t'repr': repr,\n",
    "\t\t\t\t'party': party,\n",
    "\t\t\t\t'color': color\n",
    "\t\t\t})\n",
    "\t\tjson_path = os.path.join(out_dir, filename+'.json')\n",
    "\t\twrite_json(path=json_path, dict_obj=out_dict)\n",
    "\t\n",
    "\tdef visualize(self, xs, labels, colors, parties, title, xlabel, path='plot.png'):\n",
    "\t\tunique_parties = set(parties)\n",
    "\t\tmax_x = max(xs)\n",
    "\t\tmin_x = min(xs)\n",
    "\t\tfig, axs = plt.subplots(5,2, figsize=(10,20))\n",
    "\t\taxs[0,0].scatter(xs, range(len(xs)), c =colors, alpha=0.3)\n",
    "\t\taxs[0,0].set_title(title)\n",
    "\t\taxs[0,0].set_xlabel(xlabel)\n",
    "\t\taxs[0,0].set_ylabel('Representatives')\n",
    "\t\taxs[0,0].set_xlim(int(min_x-1), int(max_x+1))\n",
    "\t\ty_ticks = axs[0,0].get_yticks()\n",
    "\t\ty_ticks_text = ['' for _ in y_ticks]\n",
    "\t\taxs[0,0].set_yticklabels(y_ticks_text)\n",
    "\t\t#Flatten axis\n",
    "\t\taxs = axs.reshape(-1)\n",
    "\t\tfor idx, party in enumerate(unique_parties):\n",
    "\t\t\tself.plot_grouped_bar_chart(axs[idx+1],\n",
    "\t\t\t\t\t\t\t   xs=[x for x, p in zip(xs, parties) if p == party],\n",
    "\t\t\t\t\t\t\t   party=party,\n",
    "\t\t\t\t\t\t\t   xmax=max_x,\n",
    "\t\t\t\t\t\t\t   xmin=min_x,\n",
    "\t\t\t\t\t\t\t   title=party,\n",
    "\t\t\t\t\t\t\t   xlabel='',\n",
    "\t\t\t\t\t\t\t   ylabel='',\n",
    "\t\t\t\t\t\t\t   color=PARTY_TO_COLOR[party])\n",
    "\n",
    "\t\tfig.tight_layout()\n",
    "\t\tplt.savefig(path)\n",
    "\t\tplt.clf()\n",
    "\t\tplt.cla()\n",
    "\t\tplt.close()\n",
    "\t\n",
    "\tdef draw_box_plot(self, data, labels=['this', 'that'], title=\"\", xlabel=\"\", ylabel=\"\", path='plot.png'):\n",
    "\t\tfig, ax = plt.subplots(figsize=(10,10))\n",
    "\t\tbplot = ax.boxplot(data, labels=[PARTYJP_TO_PARTYEN[label] for label in labels], vert=False, patch_artist=True)\n",
    "\t\tfor patch, label in zip(bplot['boxes'], labels):\n",
    "\t\t\tpatch.set_facecolor(PARTY_TO_COLOR[label])\n",
    "\t\t\tfc = patch.get_facecolor()\n",
    "\t\t\tpatch.set_facecolor((fc[0], fc[1], fc[2], 0.3))\n",
    "\t\tax.set_title(title)\n",
    "\t\tax.set_xlabel(xlabel)\n",
    "\t\tax.set_ylabel(ylabel)\n",
    "\t\tfig.tight_layout()\n",
    "\t\tplt.savefig(path)\n",
    "\t\tplt.clf()\n",
    "\t\tplt.cla()\n",
    "\t\tplt.close()\n",
    "\t\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Collapse all the other vectors onto this axis by projecting them onto the axis\n",
    "\n",
    "## 5. Create a scalar measurement for how far each politician is from the two reference points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "vo = VectorOperator()\n",
    "psv = PoliticalStanceVisualizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for topic_config in EXPERIMENT_CONFIG:\n",
    "\t# try:\n",
    "\t\ttopic = topic_config['topic_name']\n",
    "\t\tprint(f'Visualizing {topic}')\n",
    "\t\tfor_repr_name = topic_config[\"repr_references\"][\"for\"]\n",
    "\t\tagainst_repr_name = topic_config[\"repr_references\"][\"against\"]\n",
    "\t\tgen_for_sentence = topic_config[\"generated_references\"][\"for\"]\n",
    "\t\tgen_against_sentence = topic_config[\"generated_references\"][\"against\"]\n",
    "\t\tprojections, reprs = vo.collapse_vectors_onto_two_ref_reprs(summary_hdf5_path = os.path.join(DATA_REPR_SPEECHES_DIR, f'{topic}.hdf5'),\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\ttopic=topic,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tref_repr1=for_repr_name,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tref_repr2=against_repr_name)\n",
    "\t\tparties = [vo.embedding_handler.repr2party[repr] for repr in reprs]\n",
    "\n",
    "\t\tpsv.save_1d_plot(xs=projections,\n",
    "\t\t\t\t\treprs=reprs,\n",
    "\t\t\t\t\tcolors=[PARTY_TO_COLOR[party] for party in parties],\n",
    "\t\t\t\t\tparties=parties,\n",
    "\t\t\t\t\tfilename=topic+'_1d',\n",
    "\t\t\t\t\tout_dir=TODAYS_RESULTS)\n",
    "\n",
    "\t\tpsv.visualize(xs=projections,\n",
    "\t\t\t\t\tlabels=reprs,\n",
    "\t\t\t\t\tcolors=[PARTY_TO_COLOR[party] for party in parties],\n",
    "\t\t\t\t\tparties=parties,\n",
    "\t\t\t\t\ttitle=topic,\n",
    "\t\t\t\t\txlabel=f'{against_repr_name} -> {for_repr_name}',\n",
    "\t\t\t\t\tpath=os.path.join(TODAYS_PLOTS ,f'{topic}.png')\n",
    "\t\t\t\t\t)\n",
    "\t\tbox_plot_data = [[projection for idx, projection in enumerate(projections) if parties[idx] == party] for party in PARTIES if party not in IGNORE_PARTIES]\n",
    "\t\tpsv.draw_box_plot(data=box_plot_data, labels=[party for party in PARTIES if party not in IGNORE_PARTIES], title=f\"{TOPICJP_TO_TOPICEN[topic]} Representative References\", xlabel='Political Stance', ylabel='Parties', path=os.path.join(TODAYS_PLOTS ,f'{topic}_box_plot.png'))\n",
    "\t\t\n",
    "\t\tprojections, reprs = vo.collapse_vectors_onto_two_genenerated_strings(summary_hdf5_path = os.path.join(DATA_REPR_SPEECHES_DIR, f'{topic}.hdf5'),\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\ttopic=topic,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tstring1=gen_for_sentence,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tstring2=gen_against_sentence)\n",
    "\t\tparties = [vo.embedding_handler.repr2party[repr] for repr in reprs]\n",
    "\t\t\n",
    "\t\tpsv.save_1d_plot(xs=projections,\n",
    "\t\t\t\t\treprs=reprs,\n",
    "\t\t\t\t\tcolors=[PARTY_TO_COLOR[party] for party in parties],\n",
    "\t\t\t\t\tparties=parties,\n",
    "\t\t\t\t\tfilename=topic+'_gen_1d',\n",
    "\t\t\t\t\tout_dir=TODAYS_RESULTS)\n",
    "\n",
    "\t\tpsv.visualize(xs=projections,\n",
    "\t\t\t\t\tlabels=reprs,\n",
    "\t\t\t\t\tcolors=[PARTY_TO_COLOR[party] for party in parties],\n",
    "\t\t\t\t\tparties=parties,\n",
    "\t\t\t\t\ttitle=topic,\n",
    "\t\t\t\t\txlabel=f'against -> for',\n",
    "\t\t\t\t\tpath=os.path.join(TODAYS_PLOTS ,f'{topic}_gen.png')\n",
    "\t\t\t\t\t)\n",
    "\t\t\n",
    "\t\tbox_plot_data = [[projection for idx, projection in enumerate(projections) if parties[idx] == party] for party in PARTIES if party not in IGNORE_PARTIES]\n",
    "\n",
    "\t\tpsv.draw_box_plot(data=box_plot_data, labels=[party for party in PARTIES if party not in IGNORE_PARTIES], title=f\"{TOPICJP_TO_TOPICEN[topic]} Generated Reference\", xlabel='Political Stance', ylabel='Parties', path=os.path.join(TODAYS_PLOTS ,f'{topic}_gen_box_plot.png'))\n",
    "\t# except Exception as e:\n",
    "\t# \tlogger.message(f'Error visualizing {topic}')\n",
    "\t# \tlogger.message(e)\n",
    "\t\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Reduce dimensionality and visualize everything on 2D plane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for topic_config in EXPERIMENT_CONFIG:\n",
    "\t# try:\n",
    "\t\ttopic = topic_config['topic_name']\n",
    "\t\tprint(f'Visualizing {topic}')\n",
    "\t\tfor_repr_name = topic_config[\"repr_references\"][\"for\"]\n",
    "\t\tagainst_repr_name = topic_config[\"repr_references\"][\"against\"]\n",
    "\t\tgen_for_sentence = topic_config[\"generated_references\"][\"for\"]\n",
    "\t\tgen_against_sentence = topic_config[\"generated_references\"][\"against\"]\n",
    "\t\tgen_for_embedding = st.encode(gen_for_sentence, convert_to_tensor=True, show_progress_bar=True)\n",
    "\t\tgen_against_embedding = st.encode(gen_against_sentence, convert_to_tensor=True, show_progress_bar=True)\n",
    "\t\tgen_for_embedding = gen_for_embedding.cpu().numpy()\n",
    "\t\tgen_for_embedding = np.mean(gen_for_embedding, axis=0)\n",
    "\t\tgen_against_embedding = gen_against_embedding.cpu().numpy()\n",
    "\t\tgen_against_embedding = np.mean(gen_against_embedding, axis=0)\n",
    "\t\tembeddings, reprs = vo.get_embeddings_and_reprs(summary_hdf5_path = os.path.join(DATA_REPR_SPEECHES_DIR, f'{topic}.hdf5'))\n",
    "\t\textended_embeddings = np.concatenate((embeddings, [gen_for_embedding, gen_against_embedding]), axis=0)\n",
    "\t\tparties = [vo.embedding_handler.repr2party[repr] for repr in reprs]\n",
    "\t\textended_reprs = reprs + [gen_for_sentence, gen_against_sentence]\n",
    "\t\textended_parties = parties + ['', '']\n",
    "\t\tall_red_embeddings = vo.reduce_dimensions_umap(extended_embeddings, n_components=2)\n",
    "\t\tcolors = [PARTY_TO_COLOR[party] for party in parties]\n",
    "\t\t\n",
    "\t\t#first generating umap with reference representatives\n",
    "\t\tfor_repr_idx = reprs.index(for_repr_name)\n",
    "\t\tagainst_repr_idx = reprs.index(against_repr_name)\n",
    "\t\tpsv.visualize_red_dimension(\n",
    "\t\t\t\t\t\tall_red_embeddings[:-2], \n",
    "\t\t\t\t\t\treprs, \n",
    "\t\t\t\t\t\ttopic, \n",
    "\t\t\t\t\t\tparties, \n",
    "\t\t\t\t\t\tcolors, \n",
    "\t\t\t\t\t\tfor_repr_idx=for_repr_idx, \n",
    "\t\t\t\t\t\tagainst_repr_idx=against_repr_idx, \n",
    "\t\t\t\t\t\tpath=os.path.join(TODAYS_PLOTS ,f'{topic}_umap.png'), \n",
    "\t\t\t\t\t\ttitle=f'Political stance for {TOPICJP_TO_TOPICEN[topic]} with Reference Representatives', \n",
    "\t\t\t\t\t\tshow_repr_names=True)\n",
    "\t\tpsv.visualize_red_dimension(\n",
    "\t\t\t\t\t\tall_red_embeddings[:-2], \n",
    "\t\t\t\t\t\treprs, \n",
    "\t\t\t\t\t\ttopic, \n",
    "\t\t\t\t\t\tparties, \n",
    "\t\t\t\t\t\tcolors, \n",
    "\t\t\t\t\t\tfor_repr_idx=for_repr_idx, \n",
    "\t\t\t\t\t\tagainst_repr_idx=against_repr_idx, \n",
    "\t\t\t\t\t\tpath=os.path.join(TODAYS_PLOTS ,f'{topic}_umap_clean.png'), \n",
    "\t\t\t\t\t\ttitle=f'Political stance for {TOPICJP_TO_TOPICEN[topic]} with Reference representatives', \n",
    "\t\t\t\t\t\tshow_repr_names=False)\n",
    "\t\tpsv.save_2d_plot(\n",
    "\t\t\t\t\t\tred_dims=all_red_embeddings[:-2],\n",
    "\t\t\t\t\t\treprs=reprs,\n",
    "\t\t\t\t\t\tcolors=[PARTY_TO_COLOR[party] for party in parties],\n",
    "\t\t\t\t\t\tparties=parties,\n",
    "\t\t\t\t\t\tfilename=topic+'_2d',\n",
    "\t\t\t\t\t\tout_dir=TODAYS_RESULTS,\n",
    "\t\t\t\t\t\tfor_repr_idx=for_repr_idx,\n",
    "\t\t\t\t\t\tagainst_repr_idx=against_repr_idx,\n",
    "\t\t\t\t\t\tfor_repr_name=for_repr_name,\n",
    "\t\t\t\t\t\tagainst_repr_name=against_repr_name)\n",
    "\n",
    "\t\t#now generating umap with generated sentences\n",
    "\t\textended_colors = colors + ['brown', 'brown']\n",
    "\t\tgen_for_idx = extended_reprs.index(gen_for_sentence)\n",
    "\t\tgen_against_idx = extended_reprs.index(gen_against_sentence)\n",
    "\t\tpsv.visualize_red_dimension(\n",
    "\t\t\t\t\t\tall_red_embeddings, \n",
    "\t\t\t\t\t\textended_reprs, \n",
    "\t\t\t\t\t\ttopic, \n",
    "\t\t\t\t\t\textended_parties, \n",
    "\t\t\t\t\t\textended_colors, \n",
    "\t\t\t\t\t\tfor_repr_idx=gen_for_idx, \n",
    "\t\t\t\t\t\tagainst_repr_idx=gen_against_idx, \n",
    "\t\t\t\t\t\tpath=os.path.join(TODAYS_PLOTS,f'{topic}_umap_gen.png'), \n",
    "\t\t\t\t\t\ttitle=f'Political stance for {TOPICJP_TO_TOPICEN[topic]} with Generated sentences', \n",
    "\t\t\t\t\t\tshow_repr_names=False)\n",
    "\t# except Exception as e:\n",
    "\t# \tprint(e)\n",
    "\t# \tcontinue\n",
    "\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modelling on Politicians "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://qiita.com/hima2b4/items/e2fe3942b8e7253b8ed6\n",
    "import json\n",
    "import numpy as np\n",
    "import re\n",
    "import MeCab\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from bertopic import BERTopic\n",
    "import pandas as pd\n",
    "\n",
    "def create_groups(reprs, projections, parties, num_groups=3):\n",
    "    axis_range = np.max(projections) - np.min(projections)\n",
    "    portion_size = axis_range / num_groups\n",
    "    dividing_points = [np.min(projections) + i * portion_size for i in range(1, num_groups)]\n",
    "\n",
    "    groups = np.digitize(projections, dividing_points)\n",
    "\n",
    "    group_dict = [{'group_num':group_num, 'politicians': [], 'parties': []} for group_num in range(num_groups)]\n",
    "\n",
    "    for rep, group, party in zip(reprs, groups, parties):\n",
    "        group_dict[group]['politicians'].append(rep)\n",
    "        group_dict[group]['parties'].append(party)\n",
    "\n",
    "    return group_dict\n",
    "\n",
    "class TopicModeller:\n",
    "    def __init__(self,  topic=''):\n",
    "        self.embedding_handler = EmbeddingHandler()\n",
    "        self.topic = topic\n",
    "        self.reprs = self.embedding_handler.reprs\n",
    "        self.tagger = MeCab.Tagger()\n",
    "        self.tokenizer = st.tokenizer\n",
    "        self.stop_words = [word.strip() for word in list(set(read_txt_file('./stopwords-ja.txt')))]\n",
    "        self.exp_config = next((item for item in EXPERIMENT_CONFIG if item[\"topic_name\"] == topic), None)\n",
    "        self.search_words = self.exp_config[\"search_words\"]\n",
    "\n",
    "    # def create_groups(self, reprs, projections, parties, num_groups=3):\n",
    "    #     axis_range = np.max(projections) - np.min(projections)\n",
    "    #     portion_size = axis_range / num_groups\n",
    "    #     dividing_points = [np.min(projections) + i * portion_size for i in range(1, num_groups)]\n",
    "\n",
    "    #     groups = np.digitize(projections, dividing_points)\n",
    "\n",
    "    #     group_dict = [{'group_num':group_num, 'politicians': [], 'parties': []} for group_num in range(num_groups)]\n",
    "\n",
    "    #     for rep, group, party in zip(reprs, groups, parties):\n",
    "    #         group_dict[group]['politicians'].append(rep)\n",
    "    #         group_dict[group]['parties'].append(party)\n",
    "\n",
    "    #     return group_dict\n",
    "\n",
    "    def preprocess_opinion(self, text, ignore_search_words=False, ignore_stop_words=False):\n",
    "         # 改行コード、タブ、スペース削除\n",
    "        text = ''.join(text.split())\n",
    "        # URLの削除\n",
    "        text = re.sub(r'https?://[\\w/:%#\\$&\\?\\(\\)~\\.=\\+\\-]+', '', text)\n",
    "        # メンション除去 \n",
    "        text = re.sub(r'@([A-Za-z0-9_]+)', '', text) \n",
    "        # 記号の削除\n",
    "        text = re.sub(r'[!\"#$%&\\'\\\\\\\\()*+,-./:;<=>?@[\\\\]^_`{|}~「」〔〕“”〈〉『』【】＆＊・（）＄＃＠。、？！｀＋￥]', '', text)\n",
    "\n",
    "        text = re.sub('[、\\r\\n\\u3000]', '', text)\n",
    "        # stop_wordsを'|'で連結して正規表現パターンを作る\n",
    "        pattern = ''\n",
    "        if ignore_stop_words:\n",
    "            pattern = '|'.join(self.stop_words)\n",
    "            text = re.sub(pattern, '', text)\n",
    "        if ignore_search_words:\n",
    "            pattern = '|'.join(self.search_words)\n",
    "            text = re.sub(pattern, '', text)\n",
    "\n",
    "        return text\n",
    "        \n",
    "    # DO NOT USE THIS FUNCTION\n",
    "    # def extract_opinions_by_topic(self, group_dict):\n",
    "    #     group_dict_copy = group_dict.copy()\n",
    "\n",
    "    #     for group_num, group_info in group_dict.items():\n",
    "    #         opinion_sentences = []\n",
    "    #         # print('working on group number', group_num)\n",
    "    #         for politician in tqdm(group_info['politicians']):\n",
    "    #             # print('working on politician', politician)\n",
    "    #             try:\n",
    "    #                 _, _, opinions = self.embedding_handler.get_embeddings_for_repr_for_topic(politician, self.topic)\n",
    "\n",
    "    #                 opinion_sentences.extend(opinions)\n",
    "    #             except ValueError:\n",
    "    #                 print(f'No opinions for {politician} for {self.topic}')\n",
    "    #                 continue\n",
    "                    \n",
    "    #         preprocessed_opinions = self.preprocess_opinions(opinion_sentences)\n",
    "    #         group_dict_copy[group_num]['opinions_text'] = preprocessed_opinions\n",
    "\n",
    "    #     return group_dict_copy\n",
    "    \n",
    "    def get_sentences_for_politicians_for_topic(self, politicians, topic):\n",
    "        opinion_sentences = []\n",
    "        for politician in politicians:\n",
    "            try:\n",
    "                _, _, opinions = self.embedding_handler.get_embeddings_for_repr_for_topic(politician, topic)\n",
    "                opinions = [self.preprocess_opinion(opinion) for opinion in opinions]\n",
    "                opinion_sentences.extend(opinions)\n",
    "            except ValueError:\n",
    "                print(f'No opinions for {politician} for {topic}')\n",
    "                continue\n",
    "        return opinion_sentences\n",
    "    \n",
    "    # def get_topic_search_words(self) -> list:\n",
    "    #     \"\"\" \n",
    "    #     get the list of search words based on the topic \n",
    "    #     \"\"\"\n",
    "        \n",
    "    #     with open('../data_prepping/resource/experiment_config.json', encoding='utf-8') as f:\n",
    "    #         file_with_search_words = json.loads(f.read())\n",
    "        \n",
    "    #     for dic in file_with_search_words:    \n",
    "    #         if self.topic in dic[\"topic_name\"]:\n",
    "    #             return dic[\"search_words\"]\n",
    "            \n",
    "    def tokens_to_exclude(self, text, keep=[\"PROPN\", \"NOUN\"]) -> list:\n",
    "        \"\"\"\n",
    "        get all the words we want to exclude. that is, if they:\n",
    "            - aren't a certain part of speech: either proper noun (=PROPN) or noun (=NOUN) OR \n",
    "            - are a meaningless stopword (i.e., もの、こと、さん) OR\n",
    "            - are a key topic search word \n",
    "        \"\"\"\n",
    "        \n",
    "        # get tokens that are not proper nouns or nouns \n",
    "        nlp = spacy.load(\"ja_core_news_sm\")\n",
    "        doc = nlp(text)\n",
    "\n",
    "        tokens_to_remove = [token.text for token in doc if token.pos_ not in keep] + self.stop_words + self.search_words\n",
    "        return tokens_to_remove\n",
    "\n",
    "\n",
    "    def bert_tokenizer(self, text):\n",
    "        if not text:\n",
    "            return []\n",
    "        while True:\n",
    "            try:\n",
    "                text = re.sub('[、\\r\\n\\u3000]', '', text)\n",
    "                words = self.tagger.parse(text)\n",
    "                tokens = words.split('\\n')\n",
    "                tokens = [t.split('\\t')[0] for t in tokens]\n",
    "                tokens = [t for t in tokens if t != 'EOS' and t != '']\n",
    "                # tokens = self.tokenizer(text)\n",
    "                return tokens\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                logger.info(f'Preprocessed text: {text}')\n",
    "                logger.info(f'Words: {words}')\n",
    "\n",
    "\n",
    "    def apply_bertopic_to_sentences(self, sentences, nr_topics=10):\n",
    "\n",
    "        # words_removed_list = [self.bert_tokenizer(self.tokens_to_exclude(text)) for text in group_dict[group_num]['opinions_text']]\n",
    "        words_removed_list = [self.bert_tokenizer(\"\".join(self.tokens_to_exclude(text))) for text in sentences]\n",
    "        words_removed_set_list = list(set(sum(words_removed_list, [])))\n",
    "        # logger.info(f'Words removed: {words_removed_set_list}')\n",
    "\n",
    "        vectorizer_model = CountVectorizer(tokenizer=self.bert_tokenizer, stop_words=words_removed_set_list)\n",
    "\n",
    "        model = BERTopic(\n",
    "\t\t\tcalculate_probabilities=True,\n",
    "            verbose=True,\n",
    "            embedding_model=MODEL_NAME,\n",
    "            nr_topics=nr_topics,\n",
    "            vectorizer_model=vectorizer_model\n",
    "        )\n",
    "        \n",
    "        model.fit_transform(sentences)\n",
    "\n",
    "        return model\n",
    "    \n",
    "    def lda_tokenizer(self, text, keep=[\"PROPN\", \"NOUN\"]) -> list:\n",
    "        \"\"\"\n",
    "        get all the words we want to include. that is, if they:\n",
    "            - are a certain part of speech: either proper noun (=PROPN) or noun (=NOUN) OR \n",
    "            - aren't a meaningless stopword (i.e., もの、こと、さん) OR\n",
    "            - aren't a key topic search word \n",
    "\n",
    "        different from BERTopic's because of when we can access the texts and apply the filter \n",
    "        \"\"\"\n",
    "        \n",
    "        # get tokens that are not proper nouns or nouns \n",
    "        nlp = spacy.load(\"ja_core_news_sm\")\n",
    "        doc = nlp(text)\n",
    "\n",
    "        included_tokens = [token.text for token in doc if token.pos_ in keep and token.text not in self.stop_words]\n",
    "\n",
    "        return included_tokens\n",
    "\n",
    "    def apply_lda(self, group_dicts, num_topics=3):\n",
    "        print('applying lda')\n",
    "        lda_model = make_pipeline(CountVectorizer(analyzer=self.lda_tokenizer,),\n",
    "                                  LatentDirichletAllocation(n_components=num_topics, random_state=42, verbose=True))\n",
    "\n",
    "        group_topics = {}\n",
    "\n",
    "        for group_info in group_dicts:\n",
    "            opinion_sentences = []\n",
    "\n",
    "            for politician in tqdm(group_info['politicians']):\n",
    "                try:\n",
    "                    _, _, opinions = self.embedding_handler.get_embeddings_for_repr_for_topic(politician, self.topic)\n",
    "                    opinion_sentences.extend(opinions)\n",
    "                except ValueError:\n",
    "                    continue\n",
    "            print('fitting lda model')\n",
    "            \n",
    "            lda_model.fit(opinion_sentences)\n",
    "        \n",
    "            # get topics and their associated words\n",
    "            feature_names = lda_model.named_steps['countvectorizer'].get_feature_names_out()\n",
    "            topics = []\n",
    "\n",
    "            # create dict for three groups \n",
    "            print('creating dict for the topics')\n",
    "            for topic_idx, tp in enumerate(lda_model.named_steps['latentdirichletallocation'].components_):\n",
    "                topic_words = [feature_names[i] for i in tp.argsort()[:-10 - 1:-1]]\n",
    "\n",
    "                topics.append(topic_words)\n",
    "\n",
    "            group_topics[group_info['group_num']] = topics\n",
    "\n",
    "        return group_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic modelling for 防衛\n",
      "Grouped politicians by political stance\n",
      "Number of groups 3\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 42\u001b[0m\n\u001b[1;32m     40\u001b[0m opinion_sentences \u001b[38;5;241m=\u001b[39m [topic_modeller\u001b[38;5;241m.\u001b[39mpreprocess_opinion(opinion, ignore_search_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, ignore_stop_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m opinion \u001b[38;5;129;01min\u001b[39;00m opinion_sentences]\n\u001b[1;32m     41\u001b[0m opinion_sentences \u001b[38;5;241m=\u001b[39m rnd\u001b[38;5;241m.\u001b[39mchoices(opinion_sentences, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mlen\u001b[39m(opinion_sentences),\u001b[38;5;241m10000\u001b[39m))\n\u001b[0;32m---> 42\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtopic_modeller\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_bertopic_to_sentences\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopinion_sentences\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m freq \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mget_topic_info()\n\u001b[1;32m     44\u001b[0m freq\u001b[38;5;241m.\u001b[39mto_csv(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(TODAYS_RESULTS, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtopic\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_bertopic_group_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgroup_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgroup_num\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m'\u001b[39m), index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[41], line 160\u001b[0m, in \u001b[0;36mTopicModeller.apply_bertopic_to_sentences\u001b[0;34m(self, sentences, nr_topics)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_bertopic_to_sentences\u001b[39m(\u001b[38;5;28mself\u001b[39m, sentences, nr_topics\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m):\n\u001b[1;32m    158\u001b[0m \n\u001b[1;32m    159\u001b[0m     \u001b[38;5;66;03m# words_removed_list = [self.bert_tokenizer(self.tokens_to_exclude(text)) for text in group_dict[group_num]['opinions_text']]\u001b[39;00m\n\u001b[0;32m--> 160\u001b[0m     words_removed_list \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbert_tokenizer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokens_to_exclude(text))) \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m sentences]\n\u001b[1;32m    161\u001b[0m     words_removed_set_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(\u001b[38;5;28msum\u001b[39m(words_removed_list, [])))\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# logger.info(f'Words removed: {words_removed_set_list}')\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[41], line 160\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_bertopic_to_sentences\u001b[39m(\u001b[38;5;28mself\u001b[39m, sentences, nr_topics\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m):\n\u001b[1;32m    158\u001b[0m \n\u001b[1;32m    159\u001b[0m     \u001b[38;5;66;03m# words_removed_list = [self.bert_tokenizer(self.tokens_to_exclude(text)) for text in group_dict[group_num]['opinions_text']]\u001b[39;00m\n\u001b[0;32m--> 160\u001b[0m     words_removed_list \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbert_tokenizer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokens_to_exclude\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m)) \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m sentences]\n\u001b[1;32m    161\u001b[0m     words_removed_set_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(\u001b[38;5;28msum\u001b[39m(words_removed_list, [])))\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# logger.info(f'Words removed: {words_removed_set_list}')\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[41], line 132\u001b[0m, in \u001b[0;36mTopicModeller.tokens_to_exclude\u001b[0;34m(self, text, keep)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;124;03mget all the words we want to exclude. that is, if they:\u001b[39;00m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;124;03m    - aren't a certain part of speech: either proper noun (=PROPN) or noun (=NOUN) OR \u001b[39;00m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;124;03m    - are a meaningless stopword (i.e., もの、こと、さん) OR\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m    - are a key topic search word \u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;66;03m# get tokens that are not proper nouns or nouns \u001b[39;00m\n\u001b[0;32m--> 132\u001b[0m nlp \u001b[38;5;241m=\u001b[39m \u001b[43mspacy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mja_core_news_sm\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    133\u001b[0m doc \u001b[38;5;241m=\u001b[39m nlp(text)\n\u001b[1;32m    135\u001b[0m tokens_to_remove \u001b[38;5;241m=\u001b[39m [token\u001b[38;5;241m.\u001b[39mtext \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m doc \u001b[38;5;28;01mif\u001b[39;00m token\u001b[38;5;241m.\u001b[39mpos_ \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m keep] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_words \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msearch_words\n",
      "File \u001b[0;32m~/projects/kokkai_analysis/data_prepping/kokkai_venv/lib/python3.10/site-packages/spacy/__init__.py:51\u001b[0m, in \u001b[0;36mload\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\n\u001b[1;32m     28\u001b[0m     name: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     34\u001b[0m     config: Union[Dict[\u001b[38;5;28mstr\u001b[39m, Any], Config] \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mSimpleFrozenDict(),\n\u001b[1;32m     35\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Language:\n\u001b[1;32m     36\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load a spaCy model from an installed package or a local path.\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \n\u001b[1;32m     38\u001b[0m \u001b[38;5;124;03m    name (str): Package name or model path.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03m    RETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m        \u001b[49m\u001b[43menable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/kokkai_analysis/data_prepping/kokkai_venv/lib/python3.10/site-packages/spacy/util.py:465\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m    463\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m get_lang_class(name\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblank:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m))()\n\u001b[1;32m    464\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_package(name):  \u001b[38;5;66;03m# installed as package\u001b[39;00m\n\u001b[0;32m--> 465\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mload_model_from_package\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    466\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m Path(name)\u001b[38;5;241m.\u001b[39mexists():  \u001b[38;5;66;03m# path to model data directory\u001b[39;00m\n\u001b[1;32m    467\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m load_model_from_path(Path(name), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/kokkai_analysis/data_prepping/kokkai_venv/lib/python3.10/site-packages/spacy/util.py:501\u001b[0m, in \u001b[0;36mload_model_from_package\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Load a model from an installed package.\u001b[39;00m\n\u001b[1;32m    485\u001b[0m \n\u001b[1;32m    486\u001b[0m \u001b[38;5;124;03mname (str): The package name.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    498\u001b[0m \u001b[38;5;124;03mRETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[1;32m    499\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m importlib\u001b[38;5;241m.\u001b[39mimport_module(name)\n\u001b[0;32m--> 501\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/kokkai_analysis/data_prepping/kokkai_venv/lib/python3.10/site-packages/ja_core_news_sm/__init__.py:10\u001b[0m, in \u001b[0;36mload\u001b[0;34m(**overrides)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moverrides):\n\u001b[0;32m---> 10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mload_model_from_init_py\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;18;43m__file__\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moverrides\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/kokkai_analysis/data_prepping/kokkai_venv/lib/python3.10/site-packages/spacy/util.py:682\u001b[0m, in \u001b[0;36mload_model_from_init_py\u001b[0;34m(init_file, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m    680\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m model_path\u001b[38;5;241m.\u001b[39mexists():\n\u001b[1;32m    681\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE052\u001b[38;5;241m.\u001b[39mformat(path\u001b[38;5;241m=\u001b[39mdata_path))\n\u001b[0;32m--> 682\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mload_model_from_path\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    683\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    684\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    685\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmeta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmeta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    686\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdisable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    687\u001b[0m \u001b[43m    \u001b[49m\u001b[43menable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    688\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    689\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    690\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/kokkai_analysis/data_prepping/kokkai_venv/lib/python3.10/site-packages/spacy/util.py:547\u001b[0m, in \u001b[0;36mload_model_from_path\u001b[0;34m(model_path, meta, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m    538\u001b[0m config \u001b[38;5;241m=\u001b[39m load_config(config_path, overrides\u001b[38;5;241m=\u001b[39moverrides)\n\u001b[1;32m    539\u001b[0m nlp \u001b[38;5;241m=\u001b[39m load_model_from_config(\n\u001b[1;32m    540\u001b[0m     config,\n\u001b[1;32m    541\u001b[0m     vocab\u001b[38;5;241m=\u001b[39mvocab,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    545\u001b[0m     meta\u001b[38;5;241m=\u001b[39mmeta,\n\u001b[1;32m    546\u001b[0m )\n\u001b[0;32m--> 547\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnlp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_disk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverrides\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverrides\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/kokkai_analysis/data_prepping/kokkai_venv/lib/python3.10/site-packages/spacy/language.py:2184\u001b[0m, in \u001b[0;36mLanguage.from_disk\u001b[0;34m(self, path, exclude, overrides)\u001b[0m\n\u001b[1;32m   2181\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (path \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvocab\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mexists() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvocab\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m exclude:  \u001b[38;5;66;03m# type: ignore[operator]\u001b[39;00m\n\u001b[1;32m   2182\u001b[0m     \u001b[38;5;66;03m# Convert to list here in case exclude is (default) tuple\u001b[39;00m\n\u001b[1;32m   2183\u001b[0m     exclude \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(exclude) \u001b[38;5;241m+\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvocab\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m-> 2184\u001b[0m \u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_disk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeserializers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m   2185\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_path \u001b[38;5;241m=\u001b[39m path  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[1;32m   2186\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_link_components()\n",
      "File \u001b[0;32m~/projects/kokkai_analysis/data_prepping/kokkai_venv/lib/python3.10/site-packages/spacy/util.py:1372\u001b[0m, in \u001b[0;36mfrom_disk\u001b[0;34m(path, readers, exclude)\u001b[0m\n\u001b[1;32m   1369\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, reader \u001b[38;5;129;01min\u001b[39;00m readers\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   1370\u001b[0m     \u001b[38;5;66;03m# Split to support file names like meta.json\u001b[39;00m\n\u001b[1;32m   1371\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m exclude:\n\u001b[0;32m-> 1372\u001b[0m         \u001b[43mreader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m path\n",
      "File \u001b[0;32m~/projects/kokkai_analysis/data_prepping/kokkai_venv/lib/python3.10/site-packages/spacy/language.py:2178\u001b[0m, in \u001b[0;36mLanguage.from_disk.<locals>.<lambda>\u001b[0;34m(p, proc)\u001b[0m\n\u001b[1;32m   2176\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(proc, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom_disk\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   2177\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m-> 2178\u001b[0m     deserializers[name] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m p, proc\u001b[38;5;241m=\u001b[39mproc: \u001b[43mproc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_disk\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[misc]\u001b[39;49;00m\n\u001b[1;32m   2179\u001b[0m \u001b[43m        \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvocab\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m   2180\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2181\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (path \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvocab\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mexists() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvocab\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m exclude:  \u001b[38;5;66;03m# type: ignore[operator]\u001b[39;00m\n\u001b[1;32m   2182\u001b[0m     \u001b[38;5;66;03m# Convert to list here in case exclude is (default) tuple\u001b[39;00m\n\u001b[1;32m   2183\u001b[0m     exclude \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(exclude) \u001b[38;5;241m+\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvocab\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/projects/kokkai_analysis/data_prepping/kokkai_venv/lib/python3.10/site-packages/spacy/pipeline/trainable_pipe.pyx:343\u001b[0m, in \u001b[0;36mspacy.pipeline.trainable_pipe.TrainablePipe.from_disk\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/projects/kokkai_analysis/data_prepping/kokkai_venv/lib/python3.10/site-packages/spacy/util.py:1372\u001b[0m, in \u001b[0;36mfrom_disk\u001b[0;34m(path, readers, exclude)\u001b[0m\n\u001b[1;32m   1369\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, reader \u001b[38;5;129;01min\u001b[39;00m readers\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   1370\u001b[0m     \u001b[38;5;66;03m# Split to support file names like meta.json\u001b[39;00m\n\u001b[1;32m   1371\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m exclude:\n\u001b[0;32m-> 1372\u001b[0m         \u001b[43mreader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m path\n",
      "File \u001b[0;32m~/projects/kokkai_analysis/data_prepping/kokkai_venv/lib/python3.10/site-packages/spacy/pipeline/trainable_pipe.pyx:333\u001b[0m, in \u001b[0;36mspacy.pipeline.trainable_pipe.TrainablePipe.from_disk.load_model\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/projects/kokkai_analysis/data_prepping/kokkai_venv/lib/python3.10/site-packages/spacy/pipeline/trainable_pipe.pyx:334\u001b[0m, in \u001b[0;36mspacy.pipeline.trainable_pipe.TrainablePipe.from_disk.load_model\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/projects/kokkai_analysis/data_prepping/kokkai_venv/lib/python3.10/site-packages/thinc/model.py:638\u001b[0m, in \u001b[0;36mModel.from_bytes\u001b[0;34m(self, bytes_data)\u001b[0m\n\u001b[1;32m    636\u001b[0m msg \u001b[38;5;241m=\u001b[39m srsly\u001b[38;5;241m.\u001b[39mmsgpack_loads(bytes_data)\n\u001b[1;32m    637\u001b[0m msg \u001b[38;5;241m=\u001b[39m convert_recursive(is_xp_array, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39masarray, msg)\n\u001b[0;32m--> 638\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/kokkai_analysis/data_prepping/kokkai_venv/lib/python3.10/site-packages/thinc/model.py:669\u001b[0m, in \u001b[0;36mModel.from_dict\u001b[0;34m(self, msg)\u001b[0m\n\u001b[1;32m    667\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m attr, value \u001b[38;5;129;01min\u001b[39;00m msg[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattrs\u001b[39m\u001b[38;5;124m\"\u001b[39m][i]\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    668\u001b[0m     default_value \u001b[38;5;241m=\u001b[39m node\u001b[38;5;241m.\u001b[39mattrs\u001b[38;5;241m.\u001b[39mget(attr)\n\u001b[0;32m--> 669\u001b[0m     loaded_value \u001b[38;5;241m=\u001b[39m \u001b[43mdeserialize_attr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdefault_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    670\u001b[0m     node\u001b[38;5;241m.\u001b[39mattrs[attr] \u001b[38;5;241m=\u001b[39m loaded_value\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m param_name, value \u001b[38;5;129;01min\u001b[39;00m msg[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m\"\u001b[39m][i]\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m/usr/lib/python3.10/functools.py:889\u001b[0m, in \u001b[0;36msingledispatch.<locals>.wrapper\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    885\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[1;32m    886\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfuncname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires at least \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    887\u001b[0m                     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1 positional argument\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 889\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/kokkai_analysis/data_prepping/kokkai_venv/lib/python3.10/site-packages/thinc/model.py:849\u001b[0m, in \u001b[0;36mdeserialize_attr\u001b[0;34m(_, value, name, model)\u001b[0m\n\u001b[1;32m    843\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39msingledispatch\n\u001b[1;32m    844\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeserialize_attr\u001b[39m(_: Any, value: Any, name: \u001b[38;5;28mstr\u001b[39m, model: Model) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    845\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Deserialize an attribute value (defaults to msgpack). You can register\u001b[39;00m\n\u001b[1;32m    846\u001b[0m \u001b[38;5;124;03m    custom deserializers using the @deserialize_attr.register decorator with the\u001b[39;00m\n\u001b[1;32m    847\u001b[0m \u001b[38;5;124;03m    type to deserialize, e.g.: @deserialize_attr.register(MyCustomObject).\u001b[39;00m\n\u001b[1;32m    848\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 849\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msrsly\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmsgpack_loads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/kokkai_analysis/data_prepping/kokkai_venv/lib/python3.10/site-packages/srsly/_msgpack_api.py:27\u001b[0m, in \u001b[0;36mmsgpack_loads\u001b[0;34m(data, use_list)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# msgpack-python docs suggest disabling gc before unpacking large messages\u001b[39;00m\n\u001b[1;32m     26\u001b[0m gc\u001b[38;5;241m.\u001b[39mdisable()\n\u001b[0;32m---> 27\u001b[0m msg \u001b[38;5;241m=\u001b[39m \u001b[43mmsgpack\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraw\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m gc\u001b[38;5;241m.\u001b[39menable()\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m msg\n",
      "File \u001b[0;32m~/projects/kokkai_analysis/data_prepping/kokkai_venv/lib/python3.10/site-packages/srsly/msgpack/__init__.py:76\u001b[0m, in \u001b[0;36munpackb\u001b[0;34m(packed, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject_pairs_hook\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[1;32m     75\u001b[0m     object_hook \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject_hook\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 76\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m decoder \u001b[38;5;129;01min\u001b[39;00m \u001b[43mmsgpack_decoders\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[1;32m     77\u001b[0m         object_hook \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(decoder, chain\u001b[38;5;241m=\u001b[39mobject_hook)\n\u001b[1;32m     78\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject_hook\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m object_hook\n",
      "File \u001b[0;32m~/projects/kokkai_analysis/data_prepping/kokkai_venv/lib/python3.10/site-packages/catalogue/__init__.py:110\u001b[0m, in \u001b[0;36mRegistry.get_all\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    108\u001b[0m result \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mentry_points:\n\u001b[0;32m--> 110\u001b[0m     result\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_entry_points\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m keys, value \u001b[38;5;129;01min\u001b[39;00m REGISTRY\u001b[38;5;241m.\u001b[39mcopy()\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnamespace) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(keys) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\n\u001b[1;32m    113\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnamespace[i] \u001b[38;5;241m==\u001b[39m keys[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnamespace))\n\u001b[1;32m    114\u001b[0m     ):\n",
      "File \u001b[0;32m~/projects/kokkai_analysis/data_prepping/kokkai_venv/lib/python3.10/site-packages/catalogue/__init__.py:124\u001b[0m, in \u001b[0;36mRegistry.get_entry_points\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Get registered entry points from other packages for this namespace.\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \n\u001b[1;32m    121\u001b[0m \u001b[38;5;124;03mRETURNS (Dict[str, Any]): Entry points, keyed by name.\u001b[39;00m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    123\u001b[0m result \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m--> 124\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m entry_point \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_entry_points\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    125\u001b[0m     result[entry_point\u001b[38;5;241m.\u001b[39mname] \u001b[38;5;241m=\u001b[39m entry_point\u001b[38;5;241m.\u001b[39mload()\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/projects/kokkai_analysis/data_prepping/kokkai_venv/lib/python3.10/site-packages/catalogue/__init__.py:143\u001b[0m, in \u001b[0;36mRegistry._get_entry_points\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_entry_points\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[importlib_metadata\u001b[38;5;241m.\u001b[39mEntryPoint]:\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(AVAILABLE_ENTRY_POINTS, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mselect\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 143\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mAVAILABLE_ENTRY_POINTS\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mentry_point_namespace\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# dict\u001b[39;00m\n\u001b[1;32m    145\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m AVAILABLE_ENTRY_POINTS\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mentry_point_namespace, [])\n",
      "File \u001b[0;32m/usr/lib/python3.10/importlib/metadata/__init__.py:487\u001b[0m, in \u001b[0;36mSelectableGroups.select\u001b[0;34m(self, **params)\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m params:\n\u001b[1;32m    486\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[0;32m--> 487\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_all\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.10/importlib/metadata/__init__.py:359\u001b[0m, in \u001b[0;36mEntryPoints.select\u001b[0;34m(self, **params)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mselect\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams):\n\u001b[1;32m    355\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;124;03m    Select entry points from self that match the\u001b[39;00m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;124;03m    given parameters (typically group and/or name).\u001b[39;00m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 359\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mEntryPoints\u001b[49m\u001b[43m(\u001b[49m\u001b[43mep\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mep\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatches\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.10/importlib/metadata/__init__.py:359\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mselect\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams):\n\u001b[1;32m    355\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;124;03m    Select entry points from self that match the\u001b[39;00m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;124;03m    given parameters (typically group and/or name).\u001b[39;00m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 359\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m EntryPoints(ep \u001b[38;5;28;01mfor\u001b[39;00m ep \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatches\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/lib/python3.10/importlib/metadata/__init__.py:232\u001b[0m, in \u001b[0;36mEntryPoint.matches\u001b[0;34m(self, **params)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;124;03mEntryPoint matches the given parameters.\u001b[39;00m\n\u001b[1;32m    214\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;124;03mTrue\u001b[39;00m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    231\u001b[0m attrs \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, param) \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m params)\n\u001b[0;32m--> 232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28mmap\u001b[39m(operator\u001b[38;5;241m.\u001b[39meq, \u001b[43mparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, attrs))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for topic_config in EXPERIMENT_CONFIG:\n",
    "\tlogger.info(f'Topic modelling for {topic_config[\"topic_name\"]}')\n",
    "\ttopic = topic_config['topic_name']\n",
    "\tif topic not in ['原発', '防衛']:\n",
    "\t\tcontinue\n",
    "\toutput_path = os.path.join(TODAYS_RESULTS, f'{topic}_lda.json')\n",
    "\t\n",
    "\tprint(f'Topic modelling for {topic}')\n",
    "\tfor_repr_name = topic_config[\"repr_references\"][\"for\"]\n",
    "\tagainst_repr_name = topic_config[\"repr_references\"][\"against\"]\n",
    "\tgen_for_sentence = topic_config[\"generated_references\"][\"for\"]\n",
    "\tgen_against_sentence = topic_config[\"generated_references\"][\"against\"]\n",
    "\tprojections, reprs = vo.collapse_vectors_onto_two_ref_reprs(summary_hdf5_path = os.path.join(DATA_REPR_SPEECHES_DIR, f'{topic}.hdf5'),\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\ttopic=topic,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tref_repr1=for_repr_name,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tref_repr2=against_repr_name)\n",
    "\tparties = [vo.embedding_handler.repr2party[repr] for repr in reprs]\n",
    "\tgroup_dicts = create_groups(reprs, projections, parties = parties)\n",
    "\twrite_json(group_dicts, os.path.join(TODAYS_RESULTS, f'{topic}_group_dicts.json'))\n",
    "\tprint('Grouped politicians by political stance')\n",
    "\tprint('Number of groups', len(group_dicts))\n",
    "\t#######################TEST###############################\n",
    "\t# group_dict = group_dicts[1]\n",
    "\t# topic_modeller = TopicModeller(topic=topic)\n",
    "\t# politicians = group_dict['politicians']\n",
    "\t# parties = group_dict['parties']\n",
    "\t# opinion_sentences = topic_modeller.get_sentences_for_politicians_for_topic(politicians, topic)\n",
    "\t# opinion_sentences = rnd.choices(opinion_sentences, k=min(len(opinion_sentences),1000))\n",
    "\t# logger.info(f'Number of opinion sentences for {topic} for group {group_dict[\"group_num\"]}: {len(opinion_sentences)}')\n",
    "\t# model = topic_modeller.apply_bertopic_to_sentences(opinion_sentences)\n",
    "\t# freq = model.get_topic_info()\n",
    "\t# freq.to_csv(os.path.join(TODAYS_RESULTS, f'{topic}_bertopic_group_{group_dict[\"group_num\"]}.csv'), index=False)\n",
    "\t# break\n",
    "\t###################################################################\n",
    "\tfor group_dict in group_dicts:\n",
    "\t\ttopic_modeller = TopicModeller(topic=topic)\n",
    "\t\tpoliticians = group_dict['politicians']\n",
    "\t\tparties = group_dict['parties']\n",
    "\t\topinion_sentences = topic_modeller.get_sentences_for_politicians_for_topic(politicians, topic)\n",
    "\t\topinion_sentences = [topic_modeller.preprocess_opinion(opinion, ignore_search_words=False, ignore_stop_words=False) for opinion in opinion_sentences]\n",
    "\t\topinion_sentences = rnd.choices(opinion_sentences, k=min(len(opinion_sentences),10000))\n",
    "\t\tmodel = topic_modeller.apply_bertopic_to_sentences(opinion_sentences)\n",
    "\t\tfreq = model.get_topic_info()\n",
    "\t\tfreq.to_csv(os.path.join(TODAYS_RESULTS, f'{topic}_bertopic_group_{group_dict[\"group_num\"]}.csv'), index=False)\n",
    "\tlda_model = topic_modeller.apply_lda(group_dicts, 3)\n",
    "\twrite_json(path=os.path.join(TODAYS_RESULTS, f'{topic}_lda.json'), dict_obj=lda_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-10 15:51:21,454 - BERTopic - Embedding - Transforming documents to embeddings.\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.74it/s]\n",
      "2024-02-10 15:51:23,597 - BERTopic - Embedding - Completed ✓\n",
      "2024-02-10 15:51:23,598 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "/root/projects/kokkai_analysis/data_prepping/kokkai_venv/lib/python3.10/site-packages/scipy/sparse/linalg/_eigen/arpack/arpack.py:1600: RuntimeWarning: k >= N for N * N square matrix. Attempting to use scipy.linalg.eigh instead.\n",
      "  warnings.warn(\"k >= N for N * N square matrix. \"\n",
      "/root/projects/kokkai_analysis/data_prepping/kokkai_venv/lib/python3.10/site-packages/scipy/sparse/linalg/_eigen/arpack/arpack.py:1600: RuntimeWarning: k >= N for N * N square matrix. Attempting to use scipy.linalg.eigh instead.\n",
      "  warnings.warn(\"k >= N for N * N square matrix. \"\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Cannot use scipy.linalg.eigh for sparse A with k >= N. Use scipy.linalg.eigh(A.toarray()) or reduce k.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/projects/kokkai_analysis/data_prepping/kokkai_venv/lib/python3.10/site-packages/bertopic/_bertopic.py:3355\u001b[0m, in \u001b[0;36mBERTopic._reduce_dimensionality\u001b[0;34m(self, embeddings, y, partial_fit)\u001b[0m\n\u001b[1;32m   3354\u001b[0m     y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(y) \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 3355\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mumap_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3356\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n",
      "File \u001b[0;32m~/projects/kokkai_analysis/data_prepping/kokkai_venv/lib/python3.10/site-packages/umap/umap_.py:2780\u001b[0m, in \u001b[0;36mUMAP.fit\u001b[0;34m(self, X, y, force_all_finite)\u001b[0m\n\u001b[1;32m   2777\u001b[0m epochs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2778\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_epochs_list \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_epochs_list \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_epochs\n\u001b[1;32m   2779\u001b[0m )\n\u001b[0;32m-> 2780\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_, aux_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_embed_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2781\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raw_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2782\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2783\u001b[0m \u001b[43m    \u001b[49m\u001b[43minit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2784\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# JH why raw data?\u001b[39;49;00m\n\u001b[1;32m   2785\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2787\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_epochs_list \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/projects/kokkai_analysis/data_prepping/kokkai_venv/lib/python3.10/site-packages/umap/umap_.py:2826\u001b[0m, in \u001b[0;36mUMAP._fit_embed_data\u001b[0;34m(self, X, n_epochs, init, random_state)\u001b[0m\n\u001b[1;32m   2823\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"A method wrapper for simplicial_set_embedding that can be\u001b[39;00m\n\u001b[1;32m   2824\u001b[0m \u001b[38;5;124;03mreplaced by subclasses.\u001b[39;00m\n\u001b[1;32m   2825\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m-> 2826\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msimplicial_set_embedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2827\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2828\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgraph_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2829\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_components\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2830\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initial_alpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2831\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_a\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2832\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_b\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2833\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepulsion_strength\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2834\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnegative_sample_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2835\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2836\u001b[0m \u001b[43m    \u001b[49m\u001b[43minit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2837\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2838\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_distance_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2839\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_metric_kwds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2840\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdensmap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2841\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_densmap_kwds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2842\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_dens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2843\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_output_distance_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2844\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_output_metric_kwds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2845\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_metric\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meuclidean\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43ml2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2846\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom_state\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2847\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2848\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtqdm_kwds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtqdm_kwds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2849\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/kokkai_analysis/data_prepping/kokkai_venv/lib/python3.10/site-packages/umap/umap_.py:1106\u001b[0m, in \u001b[0;36msimplicial_set_embedding\u001b[0;34m(data, graph, n_components, initial_alpha, a, b, gamma, negative_sample_rate, n_epochs, init, random_state, metric, metric_kwds, densmap, densmap_kwds, output_dens, output_metric, output_metric_kwds, euclidean_output, parallel, verbose, tqdm_kwds)\u001b[0m\n\u001b[1;32m   1105\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(init, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m init \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspectral\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 1106\u001b[0m     embedding \u001b[38;5;241m=\u001b[39m \u001b[43mspectral_layout\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1107\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1108\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1109\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_components\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1110\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1112\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetric_kwds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_kwds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1113\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1114\u001b[0m     \u001b[38;5;66;03m# We add a little noise to avoid local minima for optimization to come\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/kokkai_analysis/data_prepping/kokkai_venv/lib/python3.10/site-packages/umap/spectral.py:304\u001b[0m, in \u001b[0;36mspectral_layout\u001b[0;34m(data, graph, dim, random_state, metric, metric_kwds, tol, maxiter)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;124;03mGiven a graph compute the spectral embedding of the graph. This is\u001b[39;00m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;124;03msimply the eigenvectors of the laplacian of the graph. Here we use the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;124;03m    The spectral embedding of the graph.\u001b[39;00m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 304\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_spectral_layout\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgraph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_kwds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_kwds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m    \u001b[49m\u001b[43minit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrandom\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaxiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaxiter\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/kokkai_analysis/data_prepping/kokkai_venv/lib/python3.10/site-packages/umap/spectral.py:521\u001b[0m, in \u001b[0;36m_spectral_layout\u001b[0;34m(data, graph, dim, random_state, metric, metric_kwds, init, method, tol, maxiter)\u001b[0m\n\u001b[1;32m    520\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meigsh\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 521\u001b[0m     eigenvalues, eigenvectors \u001b[38;5;241m=\u001b[39m \u001b[43mscipy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meigsh\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m        \u001b[49m\u001b[43mL\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m        \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwhich\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSM\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    525\u001b[0m \u001b[43m        \u001b[49m\u001b[43mncv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_lanczos_vectors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtol\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m1e-4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m        \u001b[49m\u001b[43mv0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones\u001b[49m\u001b[43m(\u001b[49m\u001b[43mL\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaxiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaxiter\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    530\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlobpcg\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/projects/kokkai_analysis/data_prepping/kokkai_venv/lib/python3.10/site-packages/scipy/sparse/linalg/_eigen/arpack/arpack.py:1605\u001b[0m, in \u001b[0;36meigsh\u001b[0;34m(A, k, M, sigma, which, v0, ncv, maxiter, tol, return_eigenvectors, Minv, OPinv, mode)\u001b[0m\n\u001b[1;32m   1604\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m issparse(A):\n\u001b[0;32m-> 1605\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot use scipy.linalg.eigh for sparse A with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1606\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mk >= N. Use scipy.linalg.eigh(A.toarray()) or\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1607\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m reduce k.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1608\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(A, LinearOperator):\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot use scipy.linalg.eigh for sparse A with k >= N. Use scipy.linalg.eigh(A.toarray()) or reduce k.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m fail_example \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mしたがいまして国連としてまたイラク政府としてこの航空自衛隊の支援というものに多大の感謝と継続を求めておるということは確かであります\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mこうした考え方の下に防衛力強化を裏づける防衛費の規模につきましては新たな国家安全保障戦略等の策定や今後の予算編成過程において防衛力強化の内容や財源の在り方とともに一体的に検討していくことこれが重要であると考えております\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m今のお話のとおり九条に自衛隊を書き込んでも基本的には今までと解釈は変わらないもしその発議が否決されても今までの解釈は変わらない\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      2\u001b[0m topic_modeller \u001b[38;5;241m=\u001b[39m TopicModeller(topic\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m防衛\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mtopic_modeller\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_bertopic_to_sentences\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfail_example\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[0;32mIn[12], line 174\u001b[0m, in \u001b[0;36mTopicModeller.apply_bertopic_to_sentences\u001b[0;34m(self, sentences, nr_topics)\u001b[0m\n\u001b[1;32m    164\u001b[0m         vectorizer_model \u001b[38;5;241m=\u001b[39m CountVectorizer(tokenizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbert_tokenizer, stop_words\u001b[38;5;241m=\u001b[39mwords_removed_set_list)\n\u001b[1;32m    166\u001b[0m         model \u001b[38;5;241m=\u001b[39m BERTopic(\n\u001b[1;32m    167\u001b[0m \t\t\tcalculate_probabilities\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    168\u001b[0m             verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    171\u001b[0m             vectorizer_model\u001b[38;5;241m=\u001b[39mvectorizer_model\n\u001b[1;32m    172\u001b[0m         )\n\u001b[0;32m--> 174\u001b[0m         \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentences\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    176\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/projects/kokkai_analysis/data_prepping/kokkai_venv/lib/python3.10/site-packages/bertopic/_bertopic.py:408\u001b[0m, in \u001b[0;36mBERTopic.fit_transform\u001b[0;34m(self, documents, embeddings, images, y)\u001b[0m\n\u001b[1;32m    405\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_zeroshot_topics(documents, assigned_documents, assigned_embeddings)\n\u001b[1;32m    407\u001b[0m \u001b[38;5;66;03m# Reduce dimensionality\u001b[39;00m\n\u001b[0;32m--> 408\u001b[0m umap_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reduce_dimensionality\u001b[49m\u001b[43m(\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;66;03m# Cluster reduced embeddings\u001b[39;00m\n\u001b[1;32m    411\u001b[0m documents, probabilities \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cluster_embeddings(umap_embeddings, documents, y\u001b[38;5;241m=\u001b[39my)\n",
      "File \u001b[0;32m~/projects/kokkai_analysis/data_prepping/kokkai_venv/lib/python3.10/site-packages/bertopic/_bertopic.py:3358\u001b[0m, in \u001b[0;36mBERTopic._reduce_dimensionality\u001b[0;34m(self, embeddings, y, partial_fit)\u001b[0m\n\u001b[1;32m   3355\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mumap_model\u001b[38;5;241m.\u001b[39mfit(embeddings, y\u001b[38;5;241m=\u001b[39my)\n\u001b[1;32m   3356\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m-> 3358\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mumap_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3360\u001b[0m umap_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mumap_model\u001b[38;5;241m.\u001b[39mtransform(embeddings)\n\u001b[1;32m   3361\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDimensionality - Completed \u001b[39m\u001b[38;5;130;01m\\u2713\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/projects/kokkai_analysis/data_prepping/kokkai_venv/lib/python3.10/site-packages/umap/umap_.py:2780\u001b[0m, in \u001b[0;36mUMAP.fit\u001b[0;34m(self, X, y, force_all_finite)\u001b[0m\n\u001b[1;32m   2776\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membedding\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   2777\u001b[0m     epochs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2778\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_epochs_list \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_epochs_list \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_epochs\n\u001b[1;32m   2779\u001b[0m     )\n\u001b[0;32m-> 2780\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_, aux_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_embed_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2781\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raw_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2782\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2783\u001b[0m \u001b[43m        \u001b[49m\u001b[43minit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2784\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# JH why raw data?\u001b[39;49;00m\n\u001b[1;32m   2785\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2787\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_epochs_list \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2788\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membedding_list\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m aux_data:\n",
      "File \u001b[0;32m~/projects/kokkai_analysis/data_prepping/kokkai_venv/lib/python3.10/site-packages/umap/umap_.py:2826\u001b[0m, in \u001b[0;36mUMAP._fit_embed_data\u001b[0;34m(self, X, n_epochs, init, random_state)\u001b[0m\n\u001b[1;32m   2822\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit_embed_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, n_epochs, init, random_state):\n\u001b[1;32m   2823\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"A method wrapper for simplicial_set_embedding that can be\u001b[39;00m\n\u001b[1;32m   2824\u001b[0m \u001b[38;5;124;03m    replaced by subclasses.\u001b[39;00m\n\u001b[1;32m   2825\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2826\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msimplicial_set_embedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2827\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2828\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgraph_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2829\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_components\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2830\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initial_alpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2831\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_a\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2832\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_b\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2833\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepulsion_strength\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2834\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnegative_sample_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2835\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2836\u001b[0m \u001b[43m        \u001b[49m\u001b[43minit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2837\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2838\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_distance_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2839\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_metric_kwds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2840\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdensmap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2841\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_densmap_kwds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2842\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_dens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2843\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_output_distance_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2844\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_output_metric_kwds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2845\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_metric\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meuclidean\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43ml2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2846\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom_state\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2847\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2848\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtqdm_kwds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtqdm_kwds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2849\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/kokkai_analysis/data_prepping/kokkai_venv/lib/python3.10/site-packages/umap/umap_.py:1106\u001b[0m, in \u001b[0;36msimplicial_set_embedding\u001b[0;34m(data, graph, n_components, initial_alpha, a, b, gamma, negative_sample_rate, n_epochs, init, random_state, metric, metric_kwds, densmap, densmap_kwds, output_dens, output_metric, output_metric_kwds, euclidean_output, parallel, verbose, tqdm_kwds)\u001b[0m\n\u001b[1;32m   1102\u001b[0m     embedding \u001b[38;5;241m=\u001b[39m noisy_scale_coords(\n\u001b[1;32m   1103\u001b[0m         embedding, random_state, max_coord\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, noise\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0001\u001b[39m\n\u001b[1;32m   1104\u001b[0m     )\n\u001b[1;32m   1105\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(init, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m init \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspectral\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 1106\u001b[0m     embedding \u001b[38;5;241m=\u001b[39m \u001b[43mspectral_layout\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1107\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1108\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1109\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_components\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1110\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1112\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetric_kwds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_kwds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1113\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1114\u001b[0m     \u001b[38;5;66;03m# We add a little noise to avoid local minima for optimization to come\u001b[39;00m\n\u001b[1;32m   1115\u001b[0m     embedding \u001b[38;5;241m=\u001b[39m noisy_scale_coords(\n\u001b[1;32m   1116\u001b[0m         embedding, random_state, max_coord\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, noise\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0001\u001b[39m\n\u001b[1;32m   1117\u001b[0m     )\n",
      "File \u001b[0;32m~/projects/kokkai_analysis/data_prepping/kokkai_venv/lib/python3.10/site-packages/umap/spectral.py:304\u001b[0m, in \u001b[0;36mspectral_layout\u001b[0;34m(data, graph, dim, random_state, metric, metric_kwds, tol, maxiter)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mspectral_layout\u001b[39m(\n\u001b[1;32m    264\u001b[0m     data,\n\u001b[1;32m    265\u001b[0m     graph,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    271\u001b[0m     maxiter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    272\u001b[0m ):\n\u001b[1;32m    273\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;124;03m    Given a graph compute the spectral embedding of the graph. This is\u001b[39;00m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;124;03m    simply the eigenvectors of the laplacian of the graph. Here we use the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;124;03m        The spectral embedding of the graph.\u001b[39;00m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 304\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_spectral_layout\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgraph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetric_kwds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_kwds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m        \u001b[49m\u001b[43minit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrandom\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaxiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaxiter\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/kokkai_analysis/data_prepping/kokkai_venv/lib/python3.10/site-packages/umap/spectral.py:521\u001b[0m, in \u001b[0;36m_spectral_layout\u001b[0;34m(data, graph, dim, random_state, metric, metric_kwds, init, method, tol, maxiter)\u001b[0m\n\u001b[1;32m    518\u001b[0m X[:, \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m sqrt_deg \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(sqrt_deg)\n\u001b[1;32m    520\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meigsh\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 521\u001b[0m     eigenvalues, eigenvectors \u001b[38;5;241m=\u001b[39m \u001b[43mscipy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meigsh\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m        \u001b[49m\u001b[43mL\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m        \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwhich\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSM\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    525\u001b[0m \u001b[43m        \u001b[49m\u001b[43mncv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_lanczos_vectors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtol\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m1e-4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m        \u001b[49m\u001b[43mv0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones\u001b[49m\u001b[43m(\u001b[49m\u001b[43mL\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaxiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaxiter\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    530\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlobpcg\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    531\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m warnings\u001b[38;5;241m.\u001b[39mcatch_warnings():\n",
      "File \u001b[0;32m~/projects/kokkai_analysis/data_prepping/kokkai_venv/lib/python3.10/site-packages/scipy/sparse/linalg/_eigen/arpack/arpack.py:1605\u001b[0m, in \u001b[0;36meigsh\u001b[0;34m(A, k, M, sigma, which, v0, ncv, maxiter, tol, return_eigenvectors, Minv, OPinv, mode)\u001b[0m\n\u001b[1;32m   1600\u001b[0m warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mk >= N for N * N square matrix. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1601\u001b[0m               \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempting to use scipy.linalg.eigh instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1602\u001b[0m               \u001b[38;5;167;01mRuntimeWarning\u001b[39;00m)\n\u001b[1;32m   1604\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m issparse(A):\n\u001b[0;32m-> 1605\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot use scipy.linalg.eigh for sparse A with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1606\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mk >= N. Use scipy.linalg.eigh(A.toarray()) or\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1607\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m reduce k.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1608\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(A, LinearOperator):\n\u001b[1;32m   1609\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot use scipy.linalg.eigh for LinearOperator \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1610\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA with k >= N.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot use scipy.linalg.eigh for sparse A with k >= N. Use scipy.linalg.eigh(A.toarray()) or reduce k."
     ]
    }
   ],
   "source": [
    "fail_example = ['したがいまして国連としてまたイラク政府としてこの航空自衛隊の支援というものに多大の感謝と継続を求めておるということは確かであります', 'こうした考え方の下に防衛力強化を裏づける防衛費の規模につきましては新たな国家安全保障戦略等の策定や今後の予算編成過程において防衛力強化の内容や財源の在り方とともに一体的に検討していくことこれが重要であると考えております', '今のお話のとおり九条に自衛隊を書き込んでも基本的には今までと解釈は変わらないもしその発議が否決されても今までの解釈は変わらない']\n",
    "topic_modeller = TopicModeller(topic='防衛')\n",
    "print(topic_modeller.apply_bertopic_to_sentences(fail_example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 3 # how many times we split the graph\n",
    "topic_str = '防衛'\n",
    "\n",
    "embedding_handler = EmbeddingHandler()\n",
    "topic_modeller = TopicModeller(n, \n",
    "\t\t\t\t\t\t\t   projections, \n",
    "\t\t\t\t\t\t\t   embedding_handler=embedding_handler, \n",
    "\t\t\t\t\t\t\t   topic=topic_str)\n",
    "\n",
    "group_dict = topic_modeller.create_groups()\n",
    "group_dict = topic_modeller.extract_opinions_by_topic(group_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
