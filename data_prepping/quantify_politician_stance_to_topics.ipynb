{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About the code\n",
    "This code is going to be the main to collect and create analysis output to answer the question \"Quantifying political stance of Japanese Diet members regards specific political topics through the use of LLMs and statistical methods.\" (Tentative) \n",
    "\n",
    "## Part 1: Procedure for measuring embeddings\n",
    "1. Create embeddings for each opinion-based sentence in regards to different topics and store it in a retrievable manner.\n",
    "2. Create one single \"opinion-embedding\" for each politician and store it in a retrievable manner.\n",
    "3. Create a stance axis vector by either generating two reference points or picking two points \"opinion-embeddings\" from the data\n",
    "4. Collapse all the other vectors onto this axis by projecting them onto the axis\n",
    "5. Create a scalar measurement for how far each politician is from the two reference points\n",
    "6. Use UMAP dim reduction to see the positions of the politicians as well as the generated/selected reference points\n",
    "\n",
    "## Part 2: Creating groups of politicians based on where their embeddings lie on an axis \n",
    "This is to get an idea of the ideas mentioned by politicians in a axis group within each topic using BERTopic. \n",
    "1. Divide politicians into n groups based on where they lie on the axis.\n",
    "2. Extract opinion sentences of each group made. \n",
    "3. Run each group through topic modelling techniques.\n",
    "\n",
    "### Notes\n",
    "- Data is stored under `data/data_repr` directory\n",
    "- We will attempt the procedure with different models to seek the best output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/projects/kokkai_analysis/data_prepping/kokkai_venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "DATA_DIR:  /root/projects/kokkai_analysis/data_prepping/params/../data\n",
      "DATA_REPR_SPEECHES_DIR:  /root/projects/kokkai_analysis/data_prepping/params/../data/data_repr_lower\n",
      "PARTIES:  ['自民', '国民', '公明', '立憲', 'れ新', '維新', '無', '共産', '有志']\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from sentence_transformers import models, SentenceTransformer\n",
    "import h5py\n",
    "import umap\n",
    "import numpy as np\n",
    "import torch\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from params.paths import ROOT_DIR\n",
    "import japanize_matplotlib \n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "from file_handling.file_read_writer import read_json, write_json, create_dir, write_file, read_hdf5_file, read_txt_file\n",
    "import random as rnd\n",
    "\n",
    "VERBOSE = True\n",
    "\n",
    "\n",
    "#Data Dir\n",
    "DATA_DIR = os.path.join(ROOT_DIR, 'data')\n",
    "DATA_REPR_SPEECHES_DIR = os.path.join(DATA_DIR, 'data_repr_lower')\n",
    "#Resources\n",
    "RESOURCE_DIR = os.path.join(ROOT_DIR, 'resource')\n",
    "#Results\n",
    "RESULTS_DIR = os.path.join(ROOT_DIR, 'results')\n",
    "TODAYS_RESULTS = os.path.join(RESULTS_DIR, datetime.now().strftime('%Y%m%d'))\n",
    "#Logger\n",
    "log_dir = os.path.join(TODAYS_RESULTS, 'logs')\n",
    "create_dir(log_dir)\n",
    "logging.basicConfig(filename=os.path.join(log_dir, 'quantify_politician_stance.log'), filemode='w', format='%(asctime)s - %(message)s')\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "create_dir(RESULTS_DIR)\n",
    "create_dir(TODAYS_RESULTS)\n",
    "#Plots\n",
    "TODAYS_PLOTS = os.path.join(TODAYS_RESULTS, 'plots')\n",
    "create_dir(TODAYS_PLOTS)\n",
    "#Configs\n",
    "EXPERIMENT_CONFIG_PATH = os.path.join(RESOURCE_DIR, 'experiment_config.json')\n",
    "EXPERIMENT_CONFIG = read_json(EXPERIMENT_CONFIG_PATH)\n",
    "#Other data\n",
    "PARTIES = [party for party in os.listdir(DATA_REPR_SPEECHES_DIR) if not '.' in party]\n",
    "PARTY_TO_COLOR = {\n",
    "\t'自民': 'black',\n",
    "\t'国民': 'blue',\n",
    "\t'立憲': 'orange',\n",
    "\t'公明': 'lightblue',\n",
    "\t'共産': 'red',\n",
    "\t'維新': 'gold',\n",
    "\t'れ新': 'green',\n",
    "\t'無': 'purple',\n",
    "\t'有志': 'grey'\n",
    "}\n",
    "TOPICJP_TO_TOPICEN = {\n",
    "\t'防衛': 'Defence',\n",
    "\t'原発': 'Nuclear Power',\n",
    "\t'経済': 'Economy',\n",
    "\t'気候変動': 'Climate Change',\n",
    "\t'少子化': 'Declining Birthrate',\n",
    "}\n",
    "PARTYJP_TO_PARTYEN = {\n",
    "\t'自民': 'LDP',\n",
    "\t'国民': 'NDP',\n",
    "\t'立憲': 'CDP',\n",
    "\t'公明': 'Komeito',\n",
    "\t'共産': 'JCP',\n",
    "\t'維新': 'JRP',\n",
    "\t'れ新': 'Reiwa',\n",
    "\t'無': 'None',\n",
    "\t'有志': 'Independents'\n",
    "\n",
    "}\n",
    "IGNORE_PARTIES = ['無', '有志', 'れ新']\n",
    "PARTY_TO_IDX = {party: idx for idx, party in enumerate(PARTIES)}\n",
    "IDX_TO_PARTY = {idx: party for idx, party in enumerate(PARTIES)}\n",
    "if len(PARTIES) != len(PARTY_TO_COLOR):\n",
    "\traise ValueError('PARTIES and PARTY_TO_COLOR must have the same length.')\n",
    "MODEL_NAME = \"cl-tohoku/bert-base-japanese-v3\"\n",
    "print('-----------------------------------')\n",
    "print('DATA_DIR: ', DATA_DIR)\n",
    "print('DATA_REPR_SPEECHES_DIR: ', DATA_REPR_SPEECHES_DIR)\n",
    "print('PARTIES: ', PARTIES)\n",
    "print('-----------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['特急', 'は', 'く', 'た', 'か']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape torch.Size([768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# adapted from: https://osima.jp/posts/sentence-bert/\n",
    "def fix_seed(seed=42):\n",
    "\tnp.random.seed(seed)\n",
    "\ttorch.manual_seed(seed)\n",
    "\ttorch.cuda.manual_seed(seed)\n",
    "\ttorch.backends.cudnn.deterministic = True\n",
    "\n",
    "fix_seed()\n",
    "\n",
    "sentence_transformer = models.Transformer(MODEL_NAME)\n",
    "\n",
    "pooling = models.Pooling(\n",
    "    sentence_transformer.get_word_embedding_dimension(),\n",
    "    pooling_mode_mean_tokens=False,\n",
    "    pooling_mode_cls_token=True,\n",
    "    pooling_mode_max_tokens=False)\n",
    "\n",
    "st = SentenceTransformer(modules=[sentence_transformer, pooling])\n",
    "tk = st.tokenizer\n",
    "print(tk.tokenize('特急はくたか'))\n",
    "gen_for_embedding = st.encode('これとかあれとか', convert_to_tensor=True, show_progress_bar=True)\n",
    "print('shape', gen_for_embedding.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Creating the embeddings for each opinion-based sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_speeches(speeches):\n",
    "\tencoded_opinions = st.encode(speeches, convert_to_tensor=True, show_progress_bar=True)\n",
    "\treturn encoded_opinions\n",
    "\n",
    "def read_opinion_sentences_and_dates(file_path):\n",
    "\tlogger.message(f'Reading {file_path}')\n",
    "\ttarget_dict = read_json(file_path)\n",
    "\tif not target_dict:\n",
    "\t\treturn [], []\n",
    "\topinion_sentences = []\n",
    "\tdates = []\n",
    "\tfor speech in target_dict['speeches']:\n",
    "\t\tdate = [speech['date'] for _ in range(len(speech['extracted_opinions']))]\n",
    "\t\topinions = speech['extracted_opinions']\n",
    "\t\topinion_sentences.extend(opinions)\n",
    "\t\tdates.extend(date)\n",
    "\t\n",
    "\treturn opinion_sentences, dates\n",
    "\n",
    "def iterate_topics_for_repr(repr_path, topics=[]):\n",
    "\tfor topic in os.listdir(repr_path):\n",
    "\t\tif topic not in topics and topics:\n",
    "\t\t\tcontinue\n",
    "\t\tlogger.message(f'Working on {topic}')\n",
    "\t\ttopic_path = os.path.join(repr_path, topic)\n",
    "\t\tif os.path.exists(os.path.join(topic_path, 'embeddings.hdf5')):\n",
    "\t\t\tlogger.message(f'Embeddings already exist for {topic} in {repr_path}')\n",
    "\t\t\tcontinue\n",
    "\t\tfile_path = os.path.join(topic_path, 'opinions.json')\n",
    "\t\ttopic_opinions, topic_dates = read_opinion_sentences_and_dates(file_path)\n",
    "\t\tif not topic_opinions:\n",
    "\t\t\tlogger.message(f'No opinions found for {topic} in {repr_path}')\n",
    "\t\t\tcontinue\n",
    "\t\tembeddings = embed_speeches(topic_opinions)\n",
    "\t\tembeddings = [embedding.cpu() for embedding in embeddings]\n",
    "\t\tlogger.message(f'Number of dates {len(topic_dates)}\\nNumber of opinions {len(topic_opinions)} \\nNumber of embeddings {len(embeddings)}')\n",
    "\t\tembeddings = torch.stack(embeddings)\n",
    "\t\twith h5py.File(os.path.join(topic_path, 'embeddings.hdf5'), 'w') as f:\n",
    "\t\t\tf.create_dataset('embeddings', data=embeddings)\n",
    "\t\t\tf.create_dataset('dates', data=topic_dates, dtype=h5py.string_dtype(encoding='utf-8'))\n",
    "\t\t\tf.create_dataset('opinions', data=topic_opinions, dtype=h5py.string_dtype(encoding='utf-8'))\n",
    "\n",
    "TOPICS_TO_CREATE_EMBEDDINGS = ['LGBTQ', '原発', '少子化', '気候変動', '経済対策', '防衛']\n",
    "for party in PARTIES:\n",
    "\tparty_path = os.path.join(DATA_REPR_SPEECHES_DIR, party)\n",
    "\trepr_names = os.listdir(party_path)\n",
    "\tfor repr_name in repr_names:\n",
    "\t\tlogger.message(f'{party} ----- {repr_name}')\n",
    "\t\trepr_path = os.path.join(party_path, repr_name)\n",
    "\t\titerate_topics_for_repr(repr_path, topics=TOPICS_TO_CREATE_EMBEDDINGS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create one single \"opinion-embedding\" for each politician and store it in a retrievable manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingHandler:\n",
    "\tdef __init__(self, data_dir=DATA_DIR, speeches_dir=DATA_REPR_SPEECHES_DIR, parties=PARTIES):\n",
    "\t\tself.data_dir = data_dir\n",
    "\t\tself.speeches_dir = speeches_dir\n",
    "\t\tself.parties = parties\n",
    "\t\tself.reprs = []\n",
    "\t\tself.repr2party = {}\n",
    "\t\tfor party in self.parties:\n",
    "\t\t\treprs = os.listdir(os.path.join(self.speeches_dir, party))\n",
    "\t\t\tself.reprs.extend(reprs)\n",
    "\t\t\tfor repr in reprs:\n",
    "\t\t\t\tself.repr2party[repr] = party\n",
    "\n",
    "\tdef check_party_exists(self, party):\n",
    "\t\tif party not in self.parties:\n",
    "\t\t\traise ValueError(f'{party} not in {self.parties}')\n",
    "\t\t\n",
    "\tdef get_reprs_for_party(self, party):\n",
    "\t\tself.check_party_exists(party)\n",
    "\t\treturn os.listdir(os.path.join(self.speeches_dir, party))\n",
    "\n",
    "\tdef get_topics_for_repr(self, party, repr_name):\n",
    "\t\tself.check_party_exists(party)\n",
    "\t\treturn os.listdir(os.path.join(self.speeches_dir, party, repr_name))\n",
    "\n",
    "\tdef get_embeddings_for_repr_for_topic(self,repr_name, topic, party=None):\n",
    "\t\tif party is None:\n",
    "\t\t\tparty = self.repr2party[repr_name]\n",
    "\t\tpath = os.path.join(self.speeches_dir, party, repr_name, topic, 'embeddings.hdf5')\n",
    "\t\tif not os.path.exists(path):\n",
    "\t\t\traise ValueError(f'{path} does not exist')\n",
    "\t\t\n",
    "\t\twith h5py.File(path, 'r') as f:\n",
    "\t\t\tembeddings = f['embeddings'][:]\n",
    "\t\t\tdates = [date.decode('utf-8') for date in f['dates'][:]]\n",
    "\t\t\topinions = [opinion.decode('utf-8') for opinion in f['opinions'][:]]\n",
    "\t\t\treturn embeddings, dates, opinions\n",
    "\t\t\n",
    "\tdef get_repr_speech_freq(self,repr_name, topic, party=None):\n",
    "\t\tif party is None:\n",
    "\t\t\tparty = self.repr2party[repr_name]\n",
    "\t\tpath = os.path.join(self.speeches_dir, party, repr_name, topic, 'opinions.json')\n",
    "\t\tif not os.path.exists(path):\n",
    "\t\t\traise ValueError(f'{path} does not exist')\n",
    "\t\topinions_json = read_json(path)\n",
    "\t\tif not opinions_json:\n",
    "\t\t\treturn 0\n",
    "\t\treturn len(opinions_json['speeches'])\n",
    "\t\t\n",
    "\tdef get_average_embedding_for_repr_for_topic(self, repr_name, topic, party=None):\n",
    "\t\tif party is None:\n",
    "\t\t\tparty = self.repr2party[repr_name]\n",
    "\t\tembeddings, _, _= self.get_embeddings_for_repr_for_topic(repr_name, topic, party=party)\n",
    "\t\treturn np.mean(embeddings, axis=0)\n",
    "\t\n",
    "\tdef create_summary_hdf5_file_for_average_embeddings(self, path, topic, reprs=None, ignore_repr_speech_freq_threshold=4, start_date=None, end_date=None):\n",
    "\t\tif reprs is None:\n",
    "\t\t\treprs = self.reprs\n",
    "\t\tif start_date is None:\n",
    "\t\t\tstart_date = '0000-01-01'\n",
    "\t\tif end_date is None:\n",
    "\t\t\tend_date = '9999-01-01'\n",
    "\t\tembeddings = []\n",
    "\t\treprs_with_embeddings = []\n",
    "\t\tfor repr in reprs:\n",
    "\t\t\t\tembedding_path = os.path.join(self.speeches_dir, self.repr2party[repr], repr, topic, 'embeddings.hdf5')\n",
    "\t\t\t\tif not os.path.exists(embedding_path):\n",
    "\t\t\t\t\tlogger.message(f'No embeddings for {repr} for {topic}')\n",
    "\t\t\t\t\tcontinue\n",
    "\t\t\t\tif self.get_repr_speech_freq(repr, topic) < ignore_repr_speech_freq_threshold:\n",
    "\t\t\t\t\tlogger.message(f'Ignoring {repr} because it has less than {ignore_repr_speech_freq_threshold} speeches')\n",
    "\t\t\t\t\tcontinue\n",
    "\t\t\t\taverage_embedding = self.get_average_embedding_for_repr_for_topic(repr, topic)\n",
    "\t\t\t\tembeddings.append(average_embedding)\n",
    "\t\t\t\treprs_with_embeddings.append(repr)\n",
    "\n",
    "\t\tembeddings = np.array(embeddings)\n",
    "\t\twith h5py.File(path, 'w') as f:\n",
    "\t\t\tf.create_dataset('embeddings', data=embeddings)\n",
    "\t\t\tf.create_dataset('reprs', data=reprs_with_embeddings, dtype=h5py.string_dtype(encoding='utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating summary hdf5 file for average embeddings for some topics for reprs where the data is available\n",
    "eh = EmbeddingHandler()\n",
    "TOPICS_TO_CREATE_AVERAGE_EMBEDDINGS = ['LGBTQ', '原発', '少子化', '気候変動', '経済対策', '防衛']\n",
    "for topic in TOPICS_TO_CREATE_AVERAGE_EMBEDDINGS:\n",
    "\tprint(f'Creating summary hdf5 file for all embeddings for one topic {topic}')\n",
    "\teh.create_summary_hdf5_file_for_average_embeddings(path = os.path.join(DATA_REPR_SPEECHES_DIR, f'{topic}.hdf5'),\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\ttopic = topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(eh.reprs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create a stance axis vector by either generating two reference points or picking two points \"opinion-embeddings\" from the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "class VectorOperator:\n",
    "\tdef __init__(self):\n",
    "\t\tself.embedding_handler = EmbeddingHandler()\n",
    "\t\n",
    "\tdef project_vector(vector, onto_vector):\n",
    "\t\tnormalized_onto_vector = onto_vector / np.linalg.norm(onto_vector)\n",
    "\t\tscaling = np.dot(vector, normalized_onto_vector)\n",
    "\t\tprojection = scaling * normalized_onto_vector\n",
    "\t\treturn projection, scaling\n",
    "\t\n",
    "\tdef get_embeddings_and_reprs(self, summary_hdf5_path):\n",
    "\t\thdf5_dict = read_hdf5_file(summary_hdf5_path)\n",
    "\t\tembeddings = hdf5_dict['embeddings'][:]\n",
    "\t\treprs = [repr.decode('utf-8') for repr in hdf5_dict['reprs'][:]]\n",
    "\t\treturn embeddings, reprs\n",
    "\t\n",
    "\tdef reduce_dimensions_umap(self, embeddings, n_components=2):\n",
    "\t\tumap_embeddings = umap.UMAP(n_components=n_components, verbose=True, n_neighbors=30).fit_transform(embeddings)\n",
    "\t\treturn umap_embeddings\n",
    "\n",
    "\tdef collapse_vectors_onto_two_ref_reprs(self, summary_hdf5_path, topic, ref_repr1, ref_repr2):\n",
    "\t\tembeddings, reprs = self.get_embeddings_and_reprs(summary_hdf5_path)\n",
    "\t\tif ref_repr1 not in reprs:\n",
    "\t\t\traise ValueError(f'{ref_repr1} not in {reprs}')\n",
    "\t\tif ref_repr2 not in reprs:\n",
    "\t\t\traise ValueError(f'{ref_repr2} not in {reprs}')\n",
    "\t\tref1_embedding = self.embedding_handler.get_average_embedding_for_repr_for_topic(ref_repr1, topic)\n",
    "\t\tref2_embedding = self.embedding_handler.get_average_embedding_for_repr_for_topic(ref_repr2, topic)\n",
    "\t\tref2_to_ref1 = ref1_embedding - ref2_embedding\n",
    "\t\tprojections = embeddings @ ref2_to_ref1\n",
    "\t\tprojections = projections / np.linalg.norm(ref2_to_ref1)\n",
    "\t\treturn projections, reprs\n",
    "\n",
    "\tdef collapse_vectors_onto_two_genenerated_strings(self, summary_hdf5_path, topic, string1, string2):\n",
    "\t\tembeddings, reprs = self.get_embeddings_and_reprs(summary_hdf5_path)\n",
    "\t\tstring1_embedding = st.encode(string1, convert_to_tensor=True, show_progress_bar=True)\n",
    "\t\tstring2_embedding = st.encode(string2, convert_to_tensor=True, show_progress_bar=True)\n",
    "\t\tstring1_embedding = string1_embedding.cpu().numpy()\n",
    "\t\tstring1_embedding = np.mean(string1_embedding, axis=0)\n",
    "\t\tstring2_embedding = string2_embedding.cpu().numpy()\n",
    "\t\tstring2_embedding = np.mean(string2_embedding, axis=0)\n",
    "\t\tstring1_to_string2 = string1_embedding - string2_embedding\n",
    "\t\tprojections = embeddings @ string1_to_string2\n",
    "\t\tprojections = projections / np.linalg.norm(string1_to_string2)\n",
    "\t\treturn projections, reprs\n",
    "\t\n",
    "class PoliticalStanceVisualizer:\n",
    "\tdef __init__(self):\n",
    "\t\tpass\n",
    "\n",
    "\tdef visualize_red_dimension(self, red_dims, reprs, topic, parties, colors, for_repr_idx=0, against_repr_idx=0, path='plot.png', title='', show_repr_names=False):\n",
    "\t\tfig, ax = plt.subplots(figsize=(10,10))\n",
    "\t\tax.scatter(red_dims[:,0], red_dims[:, 1], c=colors, alpha=0.3, label=parties)\n",
    "\t\tlegend_items = [Line2D([0], [0], marker='o', color='w', label=PARTYJP_TO_PARTYEN[party], markerfacecolor=color, markersize=10, alpha=0.3) for party, color in PARTY_TO_COLOR.items()]\n",
    "\t\tax.legend(handles=legend_items)\n",
    "\t\tax.scatter(red_dims[for_repr_idx, 0], red_dims[for_repr_idx, 1], edgecolors='blue', facecolors='none', s=200)\n",
    "\t\tax.scatter(red_dims[against_repr_idx, 0], red_dims[against_repr_idx, 1], edgecolors='red', facecolors='none', s=200)\n",
    "\t\tax.plot([red_dims[for_repr_idx, 0], red_dims[against_repr_idx, 0]], [red_dims[for_repr_idx, 1], red_dims[against_repr_idx, 1]], c='black')\n",
    "\t\tif show_repr_names:\n",
    "\t\t\tfor idx, repr in enumerate(reprs):\n",
    "\t\t\t\tax.annotate(repr, (red_dims[idx, 0], red_dims[idx, 1]), fontsize=7)\n",
    "\t\tax.set_title(title)\n",
    "\t\tax.set_xlabel('Red dimension 1')\n",
    "\t\tax.set_ylabel('Red dimension 2')\n",
    "\t\tfig.tight_layout()\n",
    "\t\tplt.savefig(path)\n",
    "\t\tplt.clf()\n",
    "\t\tplt.cla()\n",
    "\t\tplt.close()\n",
    "\n",
    "\tdef plot_grouped_bar_chart(self, ax, xs, party, xmax, xmin, title, xlabel='', ylabel=\"\", color=\"blue\"):\n",
    "\t\tax.set_title(title)\n",
    "\t\tax.set_xlabel(xlabel)\n",
    "\t\tax.set_ylabel(ylabel)\n",
    "\t\tax.set_xlim(int(xmin-1), int(xmax+1))\n",
    "\t\tys = []\n",
    "\t\txticks = []\n",
    "\t\tstep_size = 0.25\n",
    "\t\tfor xtick in np.arange(np.floor(xmin), np.ceil(xmax), step_size):\n",
    "\t\t\txticks.append(xtick)\n",
    "\t\t\tys.append(len([x for x in xs if (xtick-step_size/2<x<=xtick+step_size/2)]))\n",
    "\t\tax.bar(xticks, ys, color=color, alpha=0.3, width=step_size)\n",
    "\t\t\n",
    "\tdef save_2d_plot(self, red_dims, reprs, colors, parties, filename, out_dir, for_repr_name, against_repr_name, for_repr_idx, against_repr_idx):\n",
    "\t\t#save 2d hdf5 file in dir\n",
    "\t\t#save json file in dir\n",
    "\t\thdf_5_path = os.path.join(out_dir, filename+'.hdf5')\n",
    "\t\twith h5py.File(hdf_5_path, 'w') as f:\n",
    "\t\t\tf.create_dataset('red_dims', data=red_dims)\n",
    "\t\t\tf.create_dataset('reprs', data=reprs, dtype=h5py.string_dtype(encoding='utf-8'))\n",
    "\t\t\tf.create_dataset('parties', data=parties, dtype=h5py.string_dtype(encoding='utf-8'))\n",
    "\t\t\tf.create_dataset('colors', data=colors, dtype=h5py.string_dtype(encoding='utf-8'))\n",
    "\t\tout_dict = {\"data\":[]}\n",
    "\t\tfor idx, (red_dim, repr, party, color) in enumerate(zip(red_dims, reprs, parties, colors)):\n",
    "\t\t\tout_dict['data'].append({\n",
    "\t\t\t\t'idx': idx,\n",
    "\t\t\t\t'x': str(red_dim[0]),\n",
    "\t\t\t\t'y': str(red_dim[1]),\n",
    "\t\t\t\t'repr': repr,\n",
    "\t\t\t\t'party': party,\n",
    "\t\t\t\t'color': color,\n",
    "\t\t\t\t'ref_point': 'for' if repr == for_repr_name else 'against' if repr == against_repr_name else 'none'\n",
    "\t\t\t})\n",
    "\t\tjson_path = os.path.join(out_dir, filename+'.json')\n",
    "\t\twrite_json(path=json_path, dict_obj=out_dict)\n",
    "\n",
    "\tdef save_1d_plot(self, xs, reprs, colors, parties, filename, out_dir):\n",
    "\t\t#save 1d hdf5 file in dir\n",
    "\t\t#save json file in dir\n",
    "\t\thdf_5_path = os.path.join(out_dir, filename+'.hdf5')\n",
    "\t\twith h5py.File(hdf_5_path, 'w') as f:\n",
    "\t\t\tf.create_dataset('projections', data=xs)\n",
    "\t\t\tf.create_dataset('reprs', data=reprs, dtype=h5py.string_dtype(encoding='utf-8'))\n",
    "\t\t\tf.create_dataset('parties', data=parties, dtype=h5py.string_dtype(encoding='utf-8'))\n",
    "\t\t\tf.create_dataset('colors', data=colors, dtype=h5py.string_dtype(encoding='utf-8'))\n",
    "\t\tout_dict = {\"data\":[]}\n",
    "\t\tfor idx, (x, repr, party, color) in enumerate(zip(xs, reprs, parties, colors)):\n",
    "\t\t\tout_dict['data'].append({\n",
    "\t\t\t\t'idx': idx,\n",
    "\t\t\t\t'y' : str(idx),\n",
    "\t\t\t\t'x': str(x),\n",
    "\t\t\t\t'repr': repr,\n",
    "\t\t\t\t'party': party,\n",
    "\t\t\t\t'color': color\n",
    "\t\t\t})\n",
    "\t\tjson_path = os.path.join(out_dir, filename+'.json')\n",
    "\t\twrite_json(path=json_path, dict_obj=out_dict)\n",
    "\t\n",
    "\tdef visualize(self, xs, labels, colors, parties, title, xlabel, path='plot.png'):\n",
    "\t\tunique_parties = set(parties)\n",
    "\t\tmax_x = max(xs)\n",
    "\t\tmin_x = min(xs)\n",
    "\t\tfig, axs = plt.subplots(5,2, figsize=(10,20))\n",
    "\t\taxs[0,0].scatter(xs, range(len(xs)), c =colors, alpha=0.3)\n",
    "\t\taxs[0,0].set_title(title)\n",
    "\t\taxs[0,0].set_xlabel(xlabel)\n",
    "\t\taxs[0,0].set_ylabel('Representatives')\n",
    "\t\taxs[0,0].set_xlim(int(min_x-1), int(max_x+1))\n",
    "\t\ty_ticks = axs[0,0].get_yticks()\n",
    "\t\ty_ticks_text = ['' for _ in y_ticks]\n",
    "\t\taxs[0,0].set_yticklabels(y_ticks_text)\n",
    "\t\t#Flatten axis\n",
    "\t\taxs = axs.reshape(-1)\n",
    "\t\tfor idx, party in enumerate(unique_parties):\n",
    "\t\t\tself.plot_grouped_bar_chart(axs[idx+1],\n",
    "\t\t\t\t\t\t\t   xs=[x for x, p in zip(xs, parties) if p == party],\n",
    "\t\t\t\t\t\t\t   party=party,\n",
    "\t\t\t\t\t\t\t   xmax=max_x,\n",
    "\t\t\t\t\t\t\t   xmin=min_x,\n",
    "\t\t\t\t\t\t\t   title=party,\n",
    "\t\t\t\t\t\t\t   xlabel='',\n",
    "\t\t\t\t\t\t\t   ylabel='',\n",
    "\t\t\t\t\t\t\t   color=PARTY_TO_COLOR[party])\n",
    "\n",
    "\t\tfig.tight_layout()\n",
    "\t\tplt.savefig(path)\n",
    "\t\tplt.clf()\n",
    "\t\tplt.cla()\n",
    "\t\tplt.close()\n",
    "\t\n",
    "\tdef draw_box_plot(self, data, labels=['this', 'that'], title=\"\", xlabel=\"\", ylabel=\"\", path='plot.png'):\n",
    "\t\tfig, ax = plt.subplots(figsize=(10,10))\n",
    "\t\tbplot = ax.boxplot(data, labels=[PARTYJP_TO_PARTYEN[label] for label in labels], vert=False, patch_artist=True)\n",
    "\t\tfor patch, label in zip(bplot['boxes'], labels):\n",
    "\t\t\tpatch.set_facecolor(PARTY_TO_COLOR[label])\n",
    "\t\t\tfc = patch.get_facecolor()\n",
    "\t\t\tpatch.set_facecolor((fc[0], fc[1], fc[2], 0.3))\n",
    "\t\tax.set_title(title)\n",
    "\t\tax.set_xlabel(xlabel)\n",
    "\t\tax.set_ylabel(ylabel)\n",
    "\t\tfig.tight_layout()\n",
    "\t\tplt.savefig(path)\n",
    "\t\tplt.clf()\n",
    "\t\tplt.cla()\n",
    "\t\tplt.close()\n",
    "\t\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Collapse all the other vectors onto this axis by projecting them onto the axis\n",
    "\n",
    "## 5. Create a scalar measurement for how far each politician is from the two reference points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vo = VectorOperator()\n",
    "psv = PoliticalStanceVisualizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for topic_config in EXPERIMENT_CONFIG:\n",
    "\t# try:\n",
    "\t\ttopic = topic_config['topic_name']\n",
    "\t\tprint(f'Visualizing {topic}')\n",
    "\t\tfor_repr_name = topic_config[\"repr_references\"][\"for\"]\n",
    "\t\tagainst_repr_name = topic_config[\"repr_references\"][\"against\"]\n",
    "\t\tgen_for_sentence = topic_config[\"generated_references\"][\"for\"]\n",
    "\t\tgen_against_sentence = topic_config[\"generated_references\"][\"against\"]\n",
    "\t\tprojections, reprs = vo.collapse_vectors_onto_two_ref_reprs(summary_hdf5_path = os.path.join(DATA_REPR_SPEECHES_DIR, f'{topic}.hdf5'),\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\ttopic=topic,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tref_repr1=for_repr_name,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tref_repr2=against_repr_name)\n",
    "\t\tparties = [vo.embedding_handler.repr2party[repr] for repr in reprs]\n",
    "\n",
    "\t\tpsv.save_1d_plot(xs=projections,\n",
    "\t\t\t\t\treprs=reprs,\n",
    "\t\t\t\t\tcolors=[PARTY_TO_COLOR[party] for party in parties],\n",
    "\t\t\t\t\tparties=parties,\n",
    "\t\t\t\t\tfilename=topic+'_1d',\n",
    "\t\t\t\t\tout_dir=TODAYS_RESULTS)\n",
    "\n",
    "\t\tpsv.visualize(xs=projections,\n",
    "\t\t\t\t\tlabels=reprs,\n",
    "\t\t\t\t\tcolors=[PARTY_TO_COLOR[party] for party in parties],\n",
    "\t\t\t\t\tparties=parties,\n",
    "\t\t\t\t\ttitle=topic,\n",
    "\t\t\t\t\txlabel=f'{against_repr_name} -> {for_repr_name}',\n",
    "\t\t\t\t\tpath=os.path.join(TODAYS_PLOTS ,f'{topic}.png')\n",
    "\t\t\t\t\t)\n",
    "\t\tbox_plot_data = [[projection for idx, projection in enumerate(projections) if parties[idx] == party] for party in PARTIES if party not in IGNORE_PARTIES]\n",
    "\t\tpsv.draw_box_plot(data=box_plot_data, labels=[party for party in PARTIES if party not in IGNORE_PARTIES], title=f\"{TOPICJP_TO_TOPICEN[topic]} Representative References\", xlabel='Political Stance', ylabel='Parties', path=os.path.join(TODAYS_PLOTS ,f'{topic}_box_plot.png'))\n",
    "\t\t\n",
    "\t\tprojections, reprs = vo.collapse_vectors_onto_two_genenerated_strings(summary_hdf5_path = os.path.join(DATA_REPR_SPEECHES_DIR, f'{topic}.hdf5'),\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\ttopic=topic,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tstring1=gen_for_sentence,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tstring2=gen_against_sentence)\n",
    "\t\tparties = [vo.embedding_handler.repr2party[repr] for repr in reprs]\n",
    "\t\t\n",
    "\t\tpsv.save_1d_plot(xs=projections,\n",
    "\t\t\t\t\treprs=reprs,\n",
    "\t\t\t\t\tcolors=[PARTY_TO_COLOR[party] for party in parties],\n",
    "\t\t\t\t\tparties=parties,\n",
    "\t\t\t\t\tfilename=topic+'_gen_1d',\n",
    "\t\t\t\t\tout_dir=TODAYS_RESULTS)\n",
    "\n",
    "\t\tpsv.visualize(xs=projections,\n",
    "\t\t\t\t\tlabels=reprs,\n",
    "\t\t\t\t\tcolors=[PARTY_TO_COLOR[party] for party in parties],\n",
    "\t\t\t\t\tparties=parties,\n",
    "\t\t\t\t\ttitle=topic,\n",
    "\t\t\t\t\txlabel=f'against -> for',\n",
    "\t\t\t\t\tpath=os.path.join(TODAYS_PLOTS ,f'{topic}_gen.png')\n",
    "\t\t\t\t\t)\n",
    "\t\t\n",
    "\t\tbox_plot_data = [[projection for idx, projection in enumerate(projections) if parties[idx] == party] for party in PARTIES if party not in IGNORE_PARTIES]\n",
    "\n",
    "\t\tpsv.draw_box_plot(data=box_plot_data, labels=[party for party in PARTIES if party not in IGNORE_PARTIES], title=f\"{TOPICJP_TO_TOPICEN[topic]} Generated Reference\", xlabel='Political Stance', ylabel='Parties', path=os.path.join(TODAYS_PLOTS ,f'{topic}_gen_box_plot.png'))\n",
    "\t# except Exception as e:\n",
    "\t# \tlogger.message(f'Error visualizing {topic}')\n",
    "\t# \tlogger.message(e)\n",
    "\t\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Reduce dimensionality and visualize everything on 2D plane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for topic_config in EXPERIMENT_CONFIG:\n",
    "\t# try:\n",
    "\t\ttopic = topic_config['topic_name']\n",
    "\t\tprint(f'Visualizing {topic}')\n",
    "\t\tfor_repr_name = topic_config[\"repr_references\"][\"for\"]\n",
    "\t\tagainst_repr_name = topic_config[\"repr_references\"][\"against\"]\n",
    "\t\tgen_for_sentence = topic_config[\"generated_references\"][\"for\"]\n",
    "\t\tgen_against_sentence = topic_config[\"generated_references\"][\"against\"]\n",
    "\t\tgen_for_embedding = st.encode(gen_for_sentence, convert_to_tensor=True, show_progress_bar=True)\n",
    "\t\tgen_against_embedding = st.encode(gen_against_sentence, convert_to_tensor=True, show_progress_bar=True)\n",
    "\t\tgen_for_embedding = gen_for_embedding.cpu().numpy()\n",
    "\t\tgen_for_embedding = np.mean(gen_for_embedding, axis=0)\n",
    "\t\tgen_against_embedding = gen_against_embedding.cpu().numpy()\n",
    "\t\tgen_against_embedding = np.mean(gen_against_embedding, axis=0)\n",
    "\t\tembeddings, reprs = vo.get_embeddings_and_reprs(summary_hdf5_path = os.path.join(DATA_REPR_SPEECHES_DIR, f'{topic}.hdf5'))\n",
    "\t\textended_embeddings = np.concatenate((embeddings, [gen_for_embedding, gen_against_embedding]), axis=0)\n",
    "\t\tparties = [vo.embedding_handler.repr2party[repr] for repr in reprs]\n",
    "\t\textended_reprs = reprs + [gen_for_sentence, gen_against_sentence]\n",
    "\t\textended_parties = parties + ['', '']\n",
    "\t\tall_red_embeddings = vo.reduce_dimensions_umap(extended_embeddings, n_components=2)\n",
    "\t\tcolors = [PARTY_TO_COLOR[party] for party in parties]\n",
    "\t\t\n",
    "\t\t#first generating umap with reference representatives\n",
    "\t\tfor_repr_idx = reprs.index(for_repr_name)\n",
    "\t\tagainst_repr_idx = reprs.index(against_repr_name)\n",
    "\t\tpsv.visualize_red_dimension(\n",
    "\t\t\t\t\t\tall_red_embeddings[:-2], \n",
    "\t\t\t\t\t\treprs, \n",
    "\t\t\t\t\t\ttopic, \n",
    "\t\t\t\t\t\tparties, \n",
    "\t\t\t\t\t\tcolors, \n",
    "\t\t\t\t\t\tfor_repr_idx=for_repr_idx, \n",
    "\t\t\t\t\t\tagainst_repr_idx=against_repr_idx, \n",
    "\t\t\t\t\t\tpath=os.path.join(TODAYS_PLOTS ,f'{topic}_umap.png'), \n",
    "\t\t\t\t\t\ttitle=f'Political stance for {TOPICJP_TO_TOPICEN[topic]} with Reference Representatives', \n",
    "\t\t\t\t\t\tshow_repr_names=True)\n",
    "\t\tpsv.visualize_red_dimension(\n",
    "\t\t\t\t\t\tall_red_embeddings[:-2], \n",
    "\t\t\t\t\t\treprs, \n",
    "\t\t\t\t\t\ttopic, \n",
    "\t\t\t\t\t\tparties, \n",
    "\t\t\t\t\t\tcolors, \n",
    "\t\t\t\t\t\tfor_repr_idx=for_repr_idx, \n",
    "\t\t\t\t\t\tagainst_repr_idx=against_repr_idx, \n",
    "\t\t\t\t\t\tpath=os.path.join(TODAYS_PLOTS ,f'{topic}_umap_clean.png'), \n",
    "\t\t\t\t\t\ttitle=f'Political stance for {TOPICJP_TO_TOPICEN[topic]} with Reference representatives', \n",
    "\t\t\t\t\t\tshow_repr_names=False)\n",
    "\t\tpsv.save_2d_plot(\n",
    "\t\t\t\t\t\tred_dims=all_red_embeddings[:-2],\n",
    "\t\t\t\t\t\treprs=reprs,\n",
    "\t\t\t\t\t\tcolors=[PARTY_TO_COLOR[party] for party in parties],\n",
    "\t\t\t\t\t\tparties=parties,\n",
    "\t\t\t\t\t\tfilename=topic+'_2d',\n",
    "\t\t\t\t\t\tout_dir=TODAYS_RESULTS,\n",
    "\t\t\t\t\t\tfor_repr_idx=for_repr_idx,\n",
    "\t\t\t\t\t\tagainst_repr_idx=against_repr_idx,\n",
    "\t\t\t\t\t\tfor_repr_name=for_repr_name,\n",
    "\t\t\t\t\t\tagainst_repr_name=against_repr_name)\n",
    "\n",
    "\t\t#now generating umap with generated sentences\n",
    "\t\textended_colors = colors + ['brown', 'brown']\n",
    "\t\tgen_for_idx = extended_reprs.index(gen_for_sentence)\n",
    "\t\tgen_against_idx = extended_reprs.index(gen_against_sentence)\n",
    "\t\tpsv.visualize_red_dimension(\n",
    "\t\t\t\t\t\tall_red_embeddings, \n",
    "\t\t\t\t\t\textended_reprs, \n",
    "\t\t\t\t\t\ttopic, \n",
    "\t\t\t\t\t\textended_parties, \n",
    "\t\t\t\t\t\textended_colors, \n",
    "\t\t\t\t\t\tfor_repr_idx=gen_for_idx, \n",
    "\t\t\t\t\t\tagainst_repr_idx=gen_against_idx, \n",
    "\t\t\t\t\t\tpath=os.path.join(TODAYS_PLOTS,f'{topic}_umap_gen.png'), \n",
    "\t\t\t\t\t\ttitle=f'Political stance for {TOPICJP_TO_TOPICEN[topic]} with Generated sentences', \n",
    "\t\t\t\t\t\tshow_repr_names=False)\n",
    "\t# except Exception as e:\n",
    "\t# \tprint(e)\n",
    "\t# \tcontinue\n",
    "\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modelling on Politicians "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://qiita.com/hima2b4/items/e2fe3942b8e7253b8ed6\n",
    "import json\n",
    "import numpy as np\n",
    "import re\n",
    "import MeCab\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from bertopic import BERTopic\n",
    "import pandas as pd\n",
    "\n",
    "def create_groups(reprs, projections, parties, num_groups=3):\n",
    "    axis_range = np.max(projections) - np.min(projections)\n",
    "    portion_size = axis_range / num_groups\n",
    "    dividing_points = [np.min(projections) + i * portion_size for i in range(1, num_groups)]\n",
    "\n",
    "    groups = np.digitize(projections, dividing_points)\n",
    "\n",
    "    group_dict = [{'group_num':group_num, 'politicians': [], 'parties': []} for group_num in range(num_groups)]\n",
    "\n",
    "    for rep, group, party in zip(reprs, groups, parties):\n",
    "        group_dict[group]['politicians'].append(rep)\n",
    "        group_dict[group]['parties'].append(party)\n",
    "\n",
    "    return group_dict\n",
    "\n",
    "class TopicModeller:\n",
    "    def __init__(self,  topic=''):\n",
    "        self.embedding_handler = EmbeddingHandler()\n",
    "        self.topic = topic\n",
    "        self.reprs = self.embedding_handler.reprs\n",
    "        self.tagger = MeCab.Tagger()\n",
    "        self.tokenizer = st.tokenizer\n",
    "        self.stop_words = [word.strip() for word in list(set(read_txt_file('./stopwords-ja.txt')))]\n",
    "        self.exp_config = next((item for item in EXPERIMENT_CONFIG if item[\"topic_name\"] == topic), None)\n",
    "        self.search_words = self.exp_config[\"search_words\"]\n",
    "\n",
    "    # def create_groups(self, reprs, projections, parties, num_groups=3):\n",
    "    #     axis_range = np.max(projections) - np.min(projections)\n",
    "    #     portion_size = axis_range / num_groups\n",
    "    #     dividing_points = [np.min(projections) + i * portion_size for i in range(1, num_groups)]\n",
    "\n",
    "    #     groups = np.digitize(projections, dividing_points)\n",
    "\n",
    "    #     group_dict = [{'group_num':group_num, 'politicians': [], 'parties': []} for group_num in range(num_groups)]\n",
    "\n",
    "    #     for rep, group, party in zip(reprs, groups, parties):\n",
    "    #         group_dict[group]['politicians'].append(rep)\n",
    "    #         group_dict[group]['parties'].append(party)\n",
    "\n",
    "    #     return group_dict\n",
    "\n",
    "    def preprocess_opinion(self, text, ignore_search_words=False, ignore_stop_words=False):\n",
    "         # 改行コード、タブ、スペース削除\n",
    "        text = ''.join(text.split())\n",
    "        # URLの削除\n",
    "        text = re.sub(r'https?://[\\w/:%#\\$&\\?\\(\\)~\\.=\\+\\-]+', '', text)\n",
    "        # メンション除去 \n",
    "        text = re.sub(r'@([A-Za-z0-9_]+)', '', text) \n",
    "        # 記号の削除\n",
    "        text = re.sub(r'[!\"#$%&\\'\\\\\\\\()*+,-./:;<=>?@[\\\\]^_`{|}~「」〔〕“”〈〉『』【】＆＊・（）＄＃＠。、？！｀＋￥]', '', text)\n",
    "\n",
    "        text = re.sub('[、\\r\\n\\u3000]', '', text)\n",
    "        # stop_wordsを'|'で連結して正規表現パターンを作る\n",
    "        pattern = ''\n",
    "        if ignore_stop_words:\n",
    "            pattern = '|'.join(self.stop_words)\n",
    "            text = re.sub(pattern, '', text)\n",
    "        if ignore_search_words:\n",
    "            pattern = '|'.join(self.search_words)\n",
    "            text = re.sub(pattern, '', text)\n",
    "\n",
    "        return text\n",
    "        \n",
    "    # DO NOT USE THIS FUNCTION\n",
    "    # def extract_opinions_by_topic(self, group_dict):\n",
    "    #     group_dict_copy = group_dict.copy()\n",
    "\n",
    "    #     for group_num, group_info in group_dict.items():\n",
    "    #         opinion_sentences = []\n",
    "    #         # print('working on group number', group_num)\n",
    "    #         for politician in tqdm(group_info['politicians']):\n",
    "    #             # print('working on politician', politician)\n",
    "    #             try:\n",
    "    #                 _, _, opinions = self.embedding_handler.get_embeddings_for_repr_for_topic(politician, self.topic)\n",
    "\n",
    "    #                 opinion_sentences.extend(opinions)\n",
    "    #             except ValueError:\n",
    "    #                 print(f'No opinions for {politician} for {self.topic}')\n",
    "    #                 continue\n",
    "                    \n",
    "    #         preprocessed_opinions = self.preprocess_opinions(opinion_sentences)\n",
    "    #         group_dict_copy[group_num]['opinions_text'] = preprocessed_opinions\n",
    "\n",
    "    #     return group_dict_copy\n",
    "    \n",
    "    def get_sentences_for_politicians_for_topic(self, politicians, topic):\n",
    "        opinion_sentences = []\n",
    "        for politician in politicians:\n",
    "            try:\n",
    "                _, _, opinions = self.embedding_handler.get_embeddings_for_repr_for_topic(politician, topic)\n",
    "                opinions = [self.preprocess_opinion(opinion) for opinion in opinions]\n",
    "                opinion_sentences.extend(opinions)\n",
    "            except ValueError:\n",
    "                print(f'No opinions for {politician} for {topic}')\n",
    "                continue\n",
    "        return opinion_sentences\n",
    "    \n",
    "    # def get_topic_search_words(self) -> list:\n",
    "    #     \"\"\" \n",
    "    #     get the list of search words based on the topic \n",
    "    #     \"\"\"\n",
    "        \n",
    "    #     with open('../data_prepping/resource/experiment_config.json', encoding='utf-8') as f:\n",
    "    #         file_with_search_words = json.loads(f.read())\n",
    "        \n",
    "    #     for dic in file_with_search_words:    \n",
    "    #         if self.topic in dic[\"topic_name\"]:\n",
    "    #             return dic[\"search_words\"]\n",
    "            \n",
    "    def tokens_to_exclude(self, text, keep=[\"PROPN\", \"NOUN\"]) -> list:\n",
    "        \"\"\"\n",
    "        get all the words we want to exclude. that is, if they:\n",
    "            - aren't a certain part of speech: either proper noun (=PROPN) or noun (=NOUN) OR \n",
    "            - are a meaningless stopword (i.e., もの、こと、さん) OR\n",
    "            - are a key topic search word \n",
    "        \"\"\"\n",
    "        \n",
    "        # get tokens that are not proper nouns or nouns \n",
    "        nlp = spacy.load(\"ja_core_news_sm\")\n",
    "        doc = nlp(text)\n",
    "\n",
    "        tokens_to_remove = [token.text for token in doc if token.pos_ not in keep] + self.stop_words + self.search_words\n",
    "        return tokens_to_remove\n",
    "\n",
    "\n",
    "    def bert_tokenizer(self, text):\n",
    "        if not text:\n",
    "            return []\n",
    "        while True:\n",
    "            try:\n",
    "                text = re.sub('[、\\r\\n\\u3000]', '', text)\n",
    "                words = self.tagger.parse(text)\n",
    "                tokens = words.split('\\n')\n",
    "                tokens = [t.split('\\t')[0] for t in tokens]\n",
    "                tokens = [t for t in tokens if t != 'EOS' and t != '']\n",
    "                # tokens = self.tokenizer(text)\n",
    "                return tokens\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                logger.info(f'Preprocessed text: {text}')\n",
    "                logger.info(f'Words: {words}')\n",
    "\n",
    "\n",
    "    def apply_bertopic_to_sentences(self, sentences, nr_topics=10):\n",
    "\n",
    "        # words_removed_list = [self.bert_tokenizer(self.tokens_to_exclude(text)) for text in group_dict[group_num]['opinions_text']]\n",
    "        words_removed_list = [self.bert_tokenizer(\"\".join(self.tokens_to_exclude(text))) for text in sentences]\n",
    "        words_removed_set_list = list(set(sum(words_removed_list, [])))\n",
    "        # logger.info(f'Words removed: {words_removed_set_list}')\n",
    "\n",
    "        vectorizer_model = CountVectorizer(tokenizer=self.bert_tokenizer, stop_words=words_removed_set_list)\n",
    "\n",
    "        model = BERTopic(\n",
    "\t\t\tcalculate_probabilities=True,\n",
    "            verbose=True,\n",
    "            embedding_model=MODEL_NAME,\n",
    "            nr_topics=nr_topics,\n",
    "            vectorizer_model=vectorizer_model\n",
    "        )\n",
    "        \n",
    "        model.fit_transform(sentences)\n",
    "\n",
    "        return model\n",
    "    \n",
    "    def lda_tokenizer(self, text, keep=[\"PROPN\", \"NOUN\"]) -> list:\n",
    "        \"\"\"\n",
    "        get all the words we want to include. that is, if they:\n",
    "            - are a certain part of speech: either proper noun (=PROPN) or noun (=NOUN) OR \n",
    "            - aren't a meaningless stopword (i.e., もの、こと、さん) OR\n",
    "            - aren't a key topic search word \n",
    "\n",
    "        different from BERTopic's because of when we can access the texts and apply the filter \n",
    "        \"\"\"\n",
    "        \n",
    "        # get tokens that are not proper nouns or nouns \n",
    "        nlp = spacy.load(\"ja_core_news_sm\")\n",
    "        doc = nlp(text)\n",
    "\n",
    "        included_tokens = [token.text for token in doc if token.pos_ in keep and token.text not in self.stop_words]\n",
    "\n",
    "        return included_tokens\n",
    "\n",
    "    def apply_lda(self, group_dicts, num_topics=3):\n",
    "        print('applying lda')\n",
    "        lda_model = make_pipeline(CountVectorizer(analyzer=self.lda_tokenizer,),\n",
    "                                  LatentDirichletAllocation(n_components=num_topics, random_state=42, verbose=True))\n",
    "\n",
    "        group_topics = {}\n",
    "\n",
    "        for group_info in group_dicts:\n",
    "            opinion_sentences = []\n",
    "\n",
    "            for politician in tqdm(group_info['politicians']):\n",
    "                try:\n",
    "                    _, _, opinions = self.embedding_handler.get_embeddings_for_repr_for_topic(politician, self.topic)\n",
    "                    opinion_sentences.extend(opinions)\n",
    "                except ValueError:\n",
    "                    continue\n",
    "            print('fitting lda model')\n",
    "            \n",
    "            lda_model.fit(opinion_sentences)\n",
    "        \n",
    "            # get topics and their associated words\n",
    "            feature_names = lda_model.named_steps['countvectorizer'].get_feature_names_out()\n",
    "            topics = []\n",
    "\n",
    "            # create dict for three groups \n",
    "            print('creating dict for the topics')\n",
    "            for topic_idx, tp in enumerate(lda_model.named_steps['latentdirichletallocation'].components_):\n",
    "                topic_words = [feature_names[i] for i in tp.argsort()[:-10 - 1:-1]]\n",
    "\n",
    "                topics.append(topic_words)\n",
    "\n",
    "            group_topics[group_info['group_num']] = topics\n",
    "\n",
    "        return group_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic modelling for 防衛\n",
      "Grouped politicians by political stance\n",
      "Number of groups 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-11 20:14:55,158 - BERTopic - Embedding - Transforming documents to embeddings.\n",
      "Batches: 100%|██████████| 224/224 [00:17<00:00, 12.52it/s]\n",
      "2024-02-11 20:15:15,504 - BERTopic - Embedding - Completed ✓\n",
      "2024-02-11 20:15:15,505 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "/root/projects/kokkai_analysis/data_prepping/kokkai_venv/lib/python3.10/site-packages/numba/np/ufunc/parallel.py:371: NumbaWarning: \u001b[1mThe TBB threading layer requires TBB version 2021 update 6 or later i.e., TBB_INTERFACE_VERSION >= 12060. Found TBB_INTERFACE_VERSION = 12050. The TBB threading layer is disabled.\u001b[0m\n",
      "  warnings.warn(problem)\n",
      "2024-02-11 20:15:41,562 - BERTopic - Dimensionality - Completed ✓\n",
      "2024-02-11 20:15:41,564 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2024-02-11 20:15:47,739 - BERTopic - Cluster - Completed ✓\n",
      "2024-02-11 20:15:47,741 - BERTopic - Representation - Extracting topics from clusters using representation models.\n",
      "2024-02-11 20:15:50,353 - BERTopic - Representation - Completed ✓\n",
      "2024-02-11 20:15:50,354 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2024-02-11 20:15:52,421 - BERTopic - Topic reduction - Reduced number of topics from 198 to 10\n",
      "2024-02-11 20:50:48,738 - BERTopic - Embedding - Transforming documents to embeddings.\n",
      "Batches: 100%|██████████| 313/313 [00:24<00:00, 12.78it/s]\n",
      "2024-02-11 20:51:15,891 - BERTopic - Embedding - Completed ✓\n",
      "2024-02-11 20:51:15,892 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2024-02-11 20:51:29,489 - BERTopic - Dimensionality - Completed ✓\n",
      "2024-02-11 20:51:29,490 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2024-02-11 20:51:35,531 - BERTopic - Cluster - Completed ✓\n",
      "2024-02-11 20:51:35,532 - BERTopic - Representation - Extracting topics from clusters using representation models.\n",
      "2024-02-11 20:51:39,868 - BERTopic - Representation - Completed ✓\n",
      "2024-02-11 20:51:39,869 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2024-02-11 20:51:42,825 - BERTopic - Topic reduction - Reduced number of topics from 168 to 10\n",
      "2024-02-11 21:16:40,159 - BERTopic - Embedding - Transforming documents to embeddings.\n",
      "Batches: 100%|██████████| 230/230 [00:16<00:00, 13.54it/s]\n",
      "2024-02-11 21:17:01,567 - BERTopic - Embedding - Completed ✓\n",
      "2024-02-11 21:17:01,569 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2024-02-11 21:17:13,168 - BERTopic - Dimensionality - Completed ✓\n",
      "2024-02-11 21:17:13,170 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2024-02-11 21:17:21,831 - BERTopic - Cluster - Completed ✓\n",
      "2024-02-11 21:17:21,832 - BERTopic - Representation - Extracting topics from clusters using representation models.\n",
      "2024-02-11 21:17:23,816 - BERTopic - Representation - Completed ✓\n",
      "2024-02-11 21:17:23,817 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2024-02-11 21:17:25,866 - BERTopic - Topic reduction - Reduced number of topics from 225 to 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "applying lda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:00<00:00, 214.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting lda model\n",
      "iteration: 1 of max_iter: 10\n",
      "iteration: 2 of max_iter: 10\n",
      "iteration: 3 of max_iter: 10\n",
      "iteration: 4 of max_iter: 10\n",
      "iteration: 5 of max_iter: 10\n",
      "iteration: 6 of max_iter: 10\n",
      "iteration: 7 of max_iter: 10\n",
      "iteration: 8 of max_iter: 10\n",
      "iteration: 9 of max_iter: 10\n",
      "iteration: 10 of max_iter: 10\n",
      "creating dict for the topics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 246/246 [00:00<00:00, 286.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting lda model\n",
      "iteration: 1 of max_iter: 10\n",
      "iteration: 2 of max_iter: 10\n",
      "iteration: 3 of max_iter: 10\n",
      "iteration: 4 of max_iter: 10\n",
      "iteration: 5 of max_iter: 10\n",
      "iteration: 6 of max_iter: 10\n",
      "iteration: 7 of max_iter: 10\n",
      "iteration: 8 of max_iter: 10\n",
      "iteration: 9 of max_iter: 10\n",
      "iteration: 10 of max_iter: 10\n",
      "creating dict for the topics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [00:00<00:00, 597.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting lda model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1 of max_iter: 10\n",
      "iteration: 2 of max_iter: 10\n",
      "iteration: 3 of max_iter: 10\n",
      "iteration: 4 of max_iter: 10\n",
      "iteration: 5 of max_iter: 10\n",
      "iteration: 6 of max_iter: 10\n",
      "iteration: 7 of max_iter: 10\n",
      "iteration: 8 of max_iter: 10\n",
      "iteration: 9 of max_iter: 10\n",
      "iteration: 10 of max_iter: 10\n",
      "creating dict for the topics\n",
      "Topic modelling for 原発\n",
      "Grouped politicians by political stance\n",
      "Number of groups 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-11 23:09:02,896 - BERTopic - Embedding - Transforming documents to embeddings.\n",
      "Batches: 100%|██████████| 43/43 [00:02<00:00, 17.46it/s]\n",
      "2024-02-11 23:09:07,308 - BERTopic - Embedding - Completed ✓\n",
      "2024-02-11 23:09:07,309 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2024-02-11 23:09:13,352 - BERTopic - Dimensionality - Completed ✓\n",
      "2024-02-11 23:09:13,353 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2024-02-11 23:09:13,442 - BERTopic - Cluster - Completed ✓\n",
      "2024-02-11 23:09:13,443 - BERTopic - Representation - Extracting topics from clusters using representation models.\n",
      "2024-02-11 23:09:13,711 - BERTopic - Representation - Completed ✓\n",
      "2024-02-11 23:09:13,712 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2024-02-11 23:09:13,943 - BERTopic - Topic reduction - Reduced number of topics from 35 to 10\n",
      "2024-02-11 23:40:23,951 - BERTopic - Embedding - Transforming documents to embeddings.\n",
      "Batches: 100%|██████████| 295/295 [00:20<00:00, 14.43it/s]\n",
      "2024-02-11 23:40:46,258 - BERTopic - Embedding - Completed ✓\n",
      "2024-02-11 23:40:46,260 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2024-02-11 23:40:55,305 - BERTopic - Dimensionality - Completed ✓\n",
      "2024-02-11 23:40:55,307 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2024-02-11 23:41:14,795 - BERTopic - Cluster - Completed ✓\n",
      "2024-02-11 23:41:14,796 - BERTopic - Representation - Extracting topics from clusters using representation models.\n",
      "2024-02-11 23:41:17,131 - BERTopic - Representation - Completed ✓\n",
      "2024-02-11 23:41:17,132 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2024-02-11 23:41:19,220 - BERTopic - Topic reduction - Reduced number of topics from 263 to 10\n",
      "2024-02-12 00:01:07,847 - BERTopic - Embedding - Transforming documents to embeddings.\n",
      "Batches: 100%|██████████| 194/194 [00:16<00:00, 11.97it/s]\n",
      "2024-02-12 00:01:26,373 - BERTopic - Embedding - Completed ✓\n",
      "2024-02-12 00:01:26,374 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2024-02-12 00:01:33,694 - BERTopic - Dimensionality - Completed ✓\n",
      "2024-02-12 00:01:33,695 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2024-02-12 00:01:38,301 - BERTopic - Cluster - Completed ✓\n",
      "2024-02-12 00:01:38,302 - BERTopic - Representation - Extracting topics from clusters using representation models.\n",
      "2024-02-12 00:01:40,118 - BERTopic - Representation - Completed ✓\n",
      "2024-02-12 00:01:40,120 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2024-02-12 00:01:41,817 - BERTopic - Topic reduction - Reduced number of topics from 175 to 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "applying lda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:00<00:00, 1187.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting lda model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1 of max_iter: 10\n",
      "iteration: 2 of max_iter: 10\n",
      "iteration: 3 of max_iter: 10\n",
      "iteration: 4 of max_iter: 10\n",
      "iteration: 5 of max_iter: 10\n",
      "iteration: 6 of max_iter: 10\n",
      "iteration: 7 of max_iter: 10\n",
      "iteration: 8 of max_iter: 10\n",
      "iteration: 9 of max_iter: 10\n",
      "iteration: 10 of max_iter: 10\n",
      "creating dict for the topics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 152/152 [00:00<00:00, 1387.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting lda model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1 of max_iter: 10\n",
      "iteration: 2 of max_iter: 10\n",
      "iteration: 3 of max_iter: 10\n",
      "iteration: 4 of max_iter: 10\n",
      "iteration: 5 of max_iter: 10\n",
      "iteration: 6 of max_iter: 10\n",
      "iteration: 7 of max_iter: 10\n",
      "iteration: 8 of max_iter: 10\n",
      "iteration: 9 of max_iter: 10\n",
      "iteration: 10 of max_iter: 10\n",
      "creating dict for the topics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 86/86 [00:00<00:00, 1246.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting lda model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1 of max_iter: 10\n",
      "iteration: 2 of max_iter: 10\n",
      "iteration: 3 of max_iter: 10\n",
      "iteration: 4 of max_iter: 10\n",
      "iteration: 5 of max_iter: 10\n",
      "iteration: 6 of max_iter: 10\n",
      "iteration: 7 of max_iter: 10\n",
      "iteration: 8 of max_iter: 10\n",
      "iteration: 9 of max_iter: 10\n",
      "iteration: 10 of max_iter: 10\n",
      "creating dict for the topics\n"
     ]
    }
   ],
   "source": [
    "for topic_config in EXPERIMENT_CONFIG:\n",
    "\tlogger.info(f'Topic modelling for {topic_config[\"topic_name\"]}')\n",
    "\ttopic = topic_config['topic_name']\n",
    "\tif topic not in ['原発', '防衛']:\n",
    "\t\tcontinue\n",
    "\toutput_path = os.path.join(TODAYS_RESULTS, f'{topic}_lda.json')\n",
    "\t\n",
    "\tprint(f'Topic modelling for {topic}')\n",
    "\tfor_repr_name = topic_config[\"repr_references\"][\"for\"]\n",
    "\tagainst_repr_name = topic_config[\"repr_references\"][\"against\"]\n",
    "\tgen_for_sentence = topic_config[\"generated_references\"][\"for\"]\n",
    "\tgen_against_sentence = topic_config[\"generated_references\"][\"against\"]\n",
    "\tprojections, reprs = vo.collapse_vectors_onto_two_ref_reprs(summary_hdf5_path = os.path.join(DATA_REPR_SPEECHES_DIR, f'{topic}.hdf5'),\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\ttopic=topic,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tref_repr1=for_repr_name,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tref_repr2=against_repr_name)\n",
    "\tparties = [vo.embedding_handler.repr2party[repr] for repr in reprs]\n",
    "\tgroup_dicts = create_groups(reprs, projections, parties = parties)\n",
    "\twrite_json(group_dicts, os.path.join(TODAYS_RESULTS, f'{topic}_group_dicts.json'))\n",
    "\tprint('Grouped politicians by political stance')\n",
    "\tprint('Number of groups', len(group_dicts))\n",
    "\t#######################TEST###############################\n",
    "\t# group_dict = group_dicts[1]\n",
    "\t# topic_modeller = TopicModeller(topic=topic)\n",
    "\t# politicians = group_dict['politicians']\n",
    "\t# parties = group_dict['parties']\n",
    "\t# opinion_sentences = topic_modeller.get_sentences_for_politicians_for_topic(politicians, topic)\n",
    "\t# opinion_sentences = rnd.choices(opinion_sentences, k=min(len(opinion_sentences),1000))\n",
    "\t# logger.info(f'Number of opinion sentences for {topic} for group {group_dict[\"group_num\"]}: {len(opinion_sentences)}')\n",
    "\t# model = topic_modeller.apply_bertopic_to_sentences(opinion_sentences)\n",
    "\t# freq = model.get_topic_info()\n",
    "\t# freq.to_csv(os.path.join(TODAYS_RESULTS, f'{topic}_bertopic_group_{group_dict[\"group_num\"]}.csv'), index=False)\n",
    "\t# break\n",
    "\t###################################################################\n",
    "\tfor group_dict in group_dicts:\n",
    "\t\ttopic_modeller = TopicModeller(topic=topic)\n",
    "\t\tpoliticians = group_dict['politicians']\n",
    "\t\tparties = group_dict['parties']\n",
    "\t\topinion_sentences = topic_modeller.get_sentences_for_politicians_for_topic(politicians, topic)\n",
    "\t\topinion_sentences = [topic_modeller.preprocess_opinion(opinion, ignore_search_words=False, ignore_stop_words=False) for opinion in opinion_sentences]\n",
    "\t\topinion_sentences = rnd.choices(opinion_sentences, k=min(len(opinion_sentences),10000))\n",
    "\t\tmodel = topic_modeller.apply_bertopic_to_sentences(opinion_sentences)\n",
    "\t\tfreq = model.get_topic_info()\n",
    "\t\tfreq.to_csv(os.path.join(TODAYS_RESULTS, f'{topic}_bertopic_group_{group_dict[\"group_num\"]}.csv'), index=False)\n",
    "\tlda_model = topic_modeller.apply_lda(group_dicts, 3)\n",
    "\twrite_json(path=os.path.join(TODAYS_RESULTS, f'{topic}_lda.json'), dict_obj=lda_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
